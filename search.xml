<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[23.Elasticsearch(三) Springboot整合Elasticsearch]]></title>
    <url>%2F23.Elasticsearch(%E4%B8%89)%20Springboot%E6%95%B4%E5%90%88Elasticsearch.html</url>
    <content type="text"><![CDATA[准备工作依赖Elasticsearch：6.4.2(由于spring-boot-starter-data-elasticsearch最高支持6.8.+的版本，所以本人从前两章的7.1.0换成之前下载的6.4.2版本)Springboot：2.1.7.PRLEASEpom.xml：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.7.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.cz&lt;/groupId&gt; &lt;artifactId&gt;springboot-elasticsearch&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;springboot-elasticsearch&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt; &lt;version&gt;2.1.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt;application.yml：12345678spring: elasticsearch: jest: uris: http://192.168.4.109:9200 data: elasticsearch: cluster-name: my-application cluster-nodes: 192.168.4.109:9300可以更换成你自己的ip，cluster-name在elasticsearch的配置文件里可以看到。启动进入elasticsearch的bin目录，启动elasticsearch.bat。通过cmd进入elasticsearch-head-master目录，执行grunt server命令，启动插件。如果能正常访问9100和9200端口，则继续执行下面步骤。文档操作创建实体类：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697package com.cz.springbootelasticsearch.entity;import org.springframework.data.annotation.Id;import org.springframework.data.elasticsearch.annotations.Document;import org.springframework.data.elasticsearch.annotations.Field;import org.springframework.data.elasticsearch.annotations.FieldType;@Document(indexName = &quot;item&quot;,type = &quot;docs&quot;,shards = 1,replicas = 0)public class Item &#123; @Id private Long id; @Field(type = FieldType.Text, analyzer = &quot;ik_max_word&quot;) private String title; @Field(type = FieldType.Keyword) private String category; @Field(type = FieldType.Keyword) private String brand; @Field(type = FieldType.Double) private Double price; @Field(index = false, type = FieldType.Keyword) private String images; public Item() &#123; &#125; public Item(Long id, String title, String category, String brand, Double price, String images) &#123; this.id = id; this.title = title; this.category = category; this.brand = brand; this.price = price; this.images = images; &#125; public Long getId() &#123; return id; &#125; public void setId(Long id) &#123; this.id = id; &#125; public String getTitle() &#123; return title; &#125; public void setTitle(String title) &#123; this.title = title; &#125; public String getCategory() &#123; return category; &#125; public void setCategory(String category) &#123; this.category = category; &#125; public String getBrand() &#123; return brand; &#125; public void setBrand(String brand) &#123; this.brand = brand; &#125; public Double getPrice() &#123; return price; &#125; public void setPrice(Double price) &#123; this.price = price; &#125; public String getImages() &#123; return images; &#125; public void setImages(String images) &#123; this.images = images; &#125; @Override public String toString() &#123; return &quot;Item&#123;&quot; + &quot;id=&quot; + id + &quot;, title=&apos;&quot; + title + &apos;\&apos;&apos; + &quot;, category=&apos;&quot; + category + &apos;\&apos;&apos; + &quot;, brand=&apos;&quot; + brand + &apos;\&apos;&apos; + &quot;, price=&quot; + price + &quot;, images=&apos;&quot; + images + &apos;\&apos;&apos; + &apos;&#125;&apos;; &#125;&#125;Repository接口Spring Data的强大之处，在于不用作任何dao处理，自动根据方法名或类的信息进行curd操作。所以我们只需要定义接口，继续ElasticsearchRepository就可以了。代码：12345678910111213package com.cz.springbootelasticsearch.dao;import com.cz.springbootelasticsearch.entity.Item;import org.springframework.data.elasticsearch.repository.ElasticsearchRepository;import java.util.List;public interface ItemRepository extends ElasticsearchRepository&lt;Item,Long&gt; &#123; public List&lt;Item&gt; findByTitleLike(String title); public List&lt;Item&gt; findByPriceBetween(double price1,double price2);&#125;接来下创建对应的controller，service，serviceImpl，完整代码我上传到github上，会在文章末尾贴出。创建一个新对象核心代码：controller：12345678910111213141516/** * 增加 * 本人为了方便测试，所以在下面直接new对象，真实开发则是从前端接收对象，同时记得验证。 * @param item */ @PostMapping(value = &quot;/item&quot;) public void addItem(Item item)&#123; item = new Item(); item.setId(4L); item.setTitle(&quot;三星MATE20&quot;); item.setCategory(&quot;手机&quot;); item.setBrand(&quot;三星&quot;); item.setPrice(3100.00); item.setImages(&quot;http://image.baidu.com/13123.jpg&quot;); itemService.addItem(item); &#125;serviceImpl：123456@Autowiredprivate ItemRepository itemRepository;public void addItem(Item item)&#123; itemRepository.save(item);&#125;通过postman进行测试，访问127.0.0.1:9100查看，即可发现插入成功。批量新增核心代码：controller：1234567891011121314151617181920212223242526272829/** * 批量增加 * @param */@PostMapping(value = &quot;/items&quot;)public void addItemList()&#123; List&lt;Item&gt; itemList = new ArrayList&lt;&gt;(); Item item1 = new Item(); item1.setId(5L); item1.setTitle(&quot;1+7Pro&quot;); item1.setCategory(&quot;手机&quot;); item1.setBrand(&quot;1+&quot;); item1.setPrice(2800.00); item1.setImages(&quot;http://image.baidu.com/13123.jpg&quot;); itemList.add(item1); Item item2 = new Item(); item2.setId(6L); item2.setTitle(&quot;红米K20Pro&quot;); item2.setCategory(&quot;手机&quot;); item2.setBrand(&quot;红米&quot;); item2.setPrice(2500.00); item2.setImages(&quot;http://image.baidu.com/13123.jpg&quot;); itemList.add(item2); itemService.addItemList(itemList);&#125;serviceImpl：1234@Overridepublic void addItemList(List&lt;Item&gt; itemList) &#123; itemRepository.saveAll(itemList);&#125;通过Postman测试，访问127.0.0.1:9100查看，即可发现插入成功。修改elasticsearch中没有修改，它修改的原理是先删除再新增。修改和新增是同一个接口，区分的依据就是id。核心代码：controller：123456789101112131415/** * 修改：elasticsearch没有修改，它的原理就是先删除再新增，和新增用同一个接口，区别是id * @param item */@PutMapping(value = &quot;/items&quot;)public void editItem(Item item)&#123; item = new Item(); item.setId(5L); item.setTitle(&quot;1+7Pro&quot;); item.setCategory(&quot;手机&quot;); item.setBrand(&quot;一加&quot;); item.setPrice(2800.00); item.setImages(&quot;http://image.baidu.com/13123.jpg&quot;); itemService.editItem(item);&#125;serviceImpl：1234@Overridepublic void editItem(Item item) &#123; itemRepository.save(item);&#125;查询所有ElasticsearchRepository提供了一些基本的查询方法，直接调用即可。核心代码：controller：1234567891011/** * 查询所有 */@GetMapping(value = &quot;/items&quot;)public void findAllItem()&#123; Iterable&lt;Item&gt; list = itemService.findAllItem(); for (Item item:list) &#123; System.out.println(item); &#125;&#125;serviceImpl：1234@Overridepublic Iterable&lt;Item&gt; findAllItem() &#123; return itemRepository.findAll(Sort.by(&quot;price&quot;).ascending());&#125;分页查询核心代码：controller：12345678910111213141516171819/** * 分页查询 * @param page * @param size */@GetMapping(value = &quot;/items/&#123;page&#125;/&#123;size&#125;&quot;)public void findItemByPage(@PathVariable(&quot;page&quot;)int page,@PathVariable(&quot;size&quot;)int size)&#123; Page&lt;Item&gt; items = itemService.searchByPage(page,size); long total = items.getTotalElements(); System.out.println(&quot;总条数=&quot; + total); System.out.println(&quot;总页数=&quot; + items.getTotalPages()); System.out.println(&quot;当前页：&quot; + items.getNumber()); System.out.println(&quot;每页大小：&quot; + items.getSize()); for (Item item : items) &#123; System.out.println(item); &#125;&#125;serviceImpl：12345678910111213@Overridepublic Page&lt;Item&gt; searchByPage(int page, int size) &#123; //构建查询条件 NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); //添加基本的分词查询 queryBuilder.withQuery(QueryBuilders.termQuery(&quot;category&quot;,&quot;手机&quot;)); //分页 queryBuilder.withPageable(PageRequest.of(page,size)); Page&lt;Item&gt; items = itemRepository.search(queryBuilder.build()); return items;&#125;排序查找核心代码：controller：12345678910111213/** * 排序查找 * @param condition 条件 */@GetMapping(value = &quot;/items/&#123;condition&#125;&quot;)public void findItemBySort(@PathVariable(&quot;condition&quot;)String condition)&#123; Page&lt;Item&gt; items = itemService.searchBySort(condition); for (Item item : items) &#123; System.out.println(item); &#125;&#125;serviceImpl：12345678910111213@Overridepublic Page&lt;Item&gt; searchBySort(String condition) &#123; //构建查询条件 NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); //添加基本的分词查询 queryBuilder.withQuery(QueryBuilders.termQuery(&quot;category&quot;,&quot;手机&quot;)); //排序 queryBuilder.withSort(SortBuilders.fieldSort(condition).order(SortOrder.ASC)); Page&lt;Item&gt; items = itemRepository.search(queryBuilder.build()); return items;&#125;范围查询核心代码：controller：12345678910111213/** * 根据价格区间查询item * @param price1 * @param price2 */@GetMapping(value = &quot;/items/between/&#123;price1&#125;/&#123;price2&#125;&quot;)public void findItemBetweenPrice(@PathVariable(&quot;price1&quot;)double price1,@PathVariable(&quot;price2&quot;)double price2)&#123; List&lt;Item&gt; list = itemService.findItemBetweenPrice(price1,price2); for (Item item: list) &#123; System.out.println(item); &#125;&#125;serviceImpl：1234@Overridepublic List&lt;Item&gt; findItemBetweenPrice(double price1, double price2) &#123; return itemRepository.findByPriceBetween(price1,price2);&#125;删除核心代码：controller:1234567891011/** * 删除 * @param item */@DeleteMapping(value = &quot;/items&quot;)public void delete(Item item)&#123; item = new Item(); item.setId(5L); itemService.deleteItem(item);&#125;serviceImpl:1234@Overridepublic void deleteItem(Item item) &#123; itemRepository.delete(item);&#125;聚合聚合可以让我们及其方便的实现对数据的统计、分析。例如：什么品牌的手机最受欢迎？这些手机的平均价格、最高价格、最低价格？这些手机每月的销售情况如何？Elasticsearch中的聚合，包含多种类型，最常用的一个叫桶，一个叫度量。桶按照某种方式对数据进行分组，但是不进行计算，所以bucket中会嵌套另一种聚合，那就是度量。度量分组完成后，我们一般会对数据进行聚合计算，例如求平均值、最大、最小、求和。这些被称为度量。聚合为桶核心代码：controller：1234567891011121314/** * 聚合 * @param aggName * @param aggfields */@PutMapping(value = &quot;/items/agg/&#123;aggName&#125;/&#123;aggfields&#125;&quot;)public void aggItemByCondition(@PathVariable(&quot;aggName&quot;)String aggName,@PathVariable()String aggfields)&#123; List&lt;StringTerms.Bucket&gt; buckets = itemService.aggItemByCondition(aggName,aggfields); for (StringTerms.Bucket bucket : buckets)&#123; System.out.println(bucket.getKeyAsString()); System.out.println(bucket.getDocCount()); &#125;&#125;serviceImpl：123456789101112131415161718@Overridepublic List&lt;StringTerms.Bucket&gt; aggItemByCondition(String aggName,String aggfields) &#123; //构建查询条件 NativeSearchQueryBuilder queryBuilder = new NativeSearchQueryBuilder(); //不查询任何结果 queryBuilder.withSourceFilter(new FetchSourceFilter(new String[]&#123;&quot;&quot;&#125;,null)); //1.创建一个新的聚合，聚合查询为terms，聚合名称为为aggName,聚合字段为aggfields queryBuilder.addAggregation(AggregationBuilders.terms(aggName).field(aggfields)); //2.查询，需要把结果强转成AggregatedPage类型 AggregatedPage&lt;Item&gt; aggPage = (AggregatedPage&lt;Item&gt;)itemRepository.search(queryBuilder.build()); //3.解析 //3.1 从结果中取出名为brands的那个集合 //因为是利用String类型字段进行term聚合，所以结果要转为StringTerm类型 StringTerms agg = (StringTerms)aggPage.getAggregation(aggName); //3.2 获取桶 List&lt;StringTerms.Bucket&gt; buckets = agg.getBuckets(); return buckets;&#125;总结完整代码都上传到本人github仓库了，地址：https://github.com/quietAny/springboot-elasticsearch]]></content>
      <categories>
        <category>搜索引擎</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[22.Elasticsearch(二) MySQL同步数据到ES]]></title>
    <url>%2F22.Elasticsearch(%E4%BA%8C)%20MySQL%E5%90%8C%E6%AD%A5%E6%95%B0%E6%8D%AE%E5%88%B0ES.html</url>
    <content type="text"><![CDATA[技术方案本人也是在网上搜寻各种博客，总结了如下几个方案：双写优点简单灵活缺点：业务代码耦合验证如何保证双写成功同步双写会增加响应时间消息队列优点简单业务代码耦合需要保证写入MQ成功需要考虑异步造成的一致性问题(消费无需和消费时序)通过logstash-input-jdbc类似的进行同步(个人推荐使用)优点定时批量同步缺点延时如何控制当大表垂直拆分，在ES是一张表的时候不好做数据合并(可以通过sql联合查询)对数据库有一定压力(定时轮询新增数据)通过canal优点低耦合实时性较好对主库压力小，不需要回表查缺点引入canal会引入更多的组件，例如zk，增加系统复杂性如何保证canal集群的稳定性canal机器配置需要MySQL的配置差不多，不然主从同步延迟会严重。通过logstash-input-jdbc进行同步同步方式全量同步：全部将数据同步到es，通常是刚建立es，第一次同步时使用。增量同步：将后续的更新、插入记录同步到es。安装安装logstash下载地址：https://www.elastic.co/cn/downloads/logstash无需安装，解压即用。注意版本号，本人安装的和elasticsearch一样的版本7.1.0.安装ruby因为logstash-input-jdbc插件是logstash的一个插件，使用ruby语音开发，所以安装先按照ruby也是为了好使用ruby中的gem安装插件。下载地址：https://rubyinstaller.org/downloads/本人安装的是推荐版本2.5.5-1(x64),下载完成后进行安装。安装好后试下是否安装成功，打开CMD输入：然后修改gem的源，可使用以下命令：查看gem源：gem sources -l删除默认的源：gem sources –remove https：//rubygems.org/添加新的源：gem sources -a http://gems.ruby-china.org/更新完后会，还得修改Gemfile的数据源地址：gem install bundlerbundle config mirror.https://rubygems.org https://gems.ruby-china.org安装logstash-input-jdbc打开cmd，进入logstash的bin目录下，执行安装命令： .\logstash-plugin.bat install logstash-input-jdbc等一会后，成功会显示如下使用官网文档地址：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jdbc.html首先在logst的bin目录下新建一个目录，里面包括了jdbc.conf,jdbc.sql,已经mysql的驱动：jdbc.conf的配置如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445input &#123; stdin &#123; &#125; jdbc &#123; # mysql 数据库链接,test为数据库名 jdbc_connection_string =&gt; &quot;jdbc:mysql://127.0.0.1:3306/test&quot; # 用户名和密码 jdbc_user =&gt; &quot;root&quot; jdbc_password =&gt; &quot;root&quot; # 驱动 jdbc_driver_library =&gt; &quot;D:\development\logstash-7.1.0\bin\mysql-connector-java-5.1.46.jar&quot; # 驱动类名 jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot; jdbc_paging_enabled =&gt; &quot;true&quot; jdbc_page_size =&gt; &quot;50000&quot; # 执行的sql 文件路径+名称 statement_filepath =&gt; &quot;D:\development\logstash-7.1.0\bin\jdbc.sql&quot; # 设置监听间隔 各字段含义（由左至右）分、时、天、月、年，全部为*默认含义为每分钟都更新 schedule =&gt; &quot;* * * * *&quot; # 索引类型 type =&gt; &quot;jdbc&quot; &#125;&#125;filter &#123; json &#123; source =&gt; &quot;message&quot; remove_field =&gt; [&quot;message&quot;] &#125;&#125;output &#123; elasticsearch &#123; # ES的IP地址及端口 hosts =&gt; [&quot;localhost:9200&quot;] # 索引名称 index =&gt; &quot;user&quot; # 自增ID 需要关联的数据库中有有一个id字段，对应索引的id号 document_id =&gt; &quot;%&#123;id&#125;&quot; &#125; stdout &#123; # JSON格式输出 codec =&gt; json_lines &#125;&#125;上面是本人配置，读者可以改为自己的数据库配置和文件路径以及对应es的端口。jdbc.sql配置如下：1select * from user这里是本人测试要执行的sql语句，读者可以自定义。注意：这里的jdbc.sql和jdbc.conf文件编码都必须是ANSI配置完成后启动es，创建user索引。通过以下命令启动logstash：1.\logstash.bat -f .\mysql\jdbc.conf过一会后就会自动往es添加数据，这样我们就完成了mysql同步数据到es的操作了。增量新增需要在jdbc.conf配置文件中做如下修改：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455input &#123; stdin &#123; &#125; jdbc &#123; # mysql 数据库链接,test为数据库名 jdbc_connection_string =&gt; &quot;jdbc:mysql://127.0.0.1:3306/test&quot; # 用户名和密码 jdbc_user =&gt; &quot;root&quot; jdbc_password =&gt; &quot;root&quot; # 驱动 jdbc_driver_library =&gt; &quot;D:\development\logstash-7.1.0\bin\mysql-connector-java-5.1.46.jar&quot; # 驱动类名 jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot; #处理中文乱码问题 codec =&gt; plain &#123; charset =&gt; &quot;UTF-8&quot;&#125; #使用其它字段追踪，而不是用时间 use_column_value =&gt; true #追踪的字段 tracking_column =&gt; id record_last_run =&gt; true #上一个sql_last_value值的存放文件路径, 必须要在文件中指定字段的初始值 last_run_metadata_path =&gt; &quot;D:\development\logstash-7.1.0\bin\station_parameter.txt&quot; #开启分页查询 jdbc_paging_enabled =&gt; &quot;true&quot; jdbc_page_size =&gt; &quot;50000&quot; # 执行的sql 文件路径+名称 statement_filepath =&gt; &quot;D:\development\logstash-7.1.0\bin\jdbc.sql&quot; # 设置监听间隔 各字段含义（由左至右）分、时、天、月、年，全部为*默认含义为每分钟都更新 schedule =&gt; &quot;* * * * *&quot; # 索引类型 type =&gt; &quot;jdbc&quot; &#125;&#125;filter &#123; json &#123; source =&gt; &quot;message&quot; remove_field =&gt; [&quot;message&quot;] &#125;&#125;output &#123; elasticsearch &#123; # ES的IP地址及端口 hosts =&gt; [&quot;localhost:9200&quot;] # 索引名称 index =&gt; &quot;user&quot; # 自增ID 需要关联的数据库中有有一个id字段，对应索引的id号 document_id =&gt; &quot;%&#123;id&#125;&quot; &#125; stdout &#123; # JSON格式输出 codec =&gt; json_lines &#125;&#125;打开bin目录下的station_parameter.txt文件这个文件里记录上次执行到的 tracking_column 字段的值,比如上次数据库有 10000 条记录,查询完后该文件中就会有数字 10000 这样的记录,下次执行 SQL 查询可以从 10001 条处开始,我们只需要在 SQL 语句中 WHERE MY_ID &gt; :last_sql_value 即可. 其中 :last_sql_value 取得就是该文件中的值。然后开启爬虫,爬取数据,往数据库里插，logstash会自动的识别到更新，然后导入到ES中！！总结如上文件都是我个人的配置，读者可以根据需求更换成自己的配置。思考现在我们解决了mysql同步数据到es的问题，但是我们遇到了新的问题，如何在项目中使用中，下一章我将告诉大家，敬请期待！]]></content>
      <categories>
        <category>搜索引擎</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[21.Elasticsearch(一) 介绍和安装]]></title>
    <url>%2F21.Elasticsearch(%E4%B8%80)%20%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%AE%89%E8%A3%85.html</url>
    <content type="text"><![CDATA[简介基于Lucene的搜索服务器，一个分布式多用户能力的全文搜索引擎，基于RESTful web接口，是当前流行的企业级搜索引擎。官网: https://www.elastic.co/cn/Elastic有一条完整的产品线：Elasticsearch、Kibana、Logstash等，前面说的三个就是大家常说的ELK技术栈。特点：分布式,无需人工搭建集群Restful风格，一切API都遵循Rest原则，容易上手近实时搜索，数据更新在Elasticsearch中几乎是完全同步的。安装版本：Elasticsearch：7.1.0(最新版为7.3.0，可自行选择)。JDK: 1.8下载地址：https://www.elastic.co/cn/downloads/elasticsearch无需安装，解压即用运行进入elasticsearch/bin 目录，可以看到执行文件双击启动后绑定了两个端口：9300:java程序访问的端口9200:浏览器，postman访问端口安装Head插件什么是Head?一个专门针对于elasticsearch的客户端工具。下载地址：https://github.com/mobz/elasticsearch-head安装es5以上的版本安装head需要安装node和grunt第一步：从地址：https://nodejs.org/en/download/ 下载相应系统的msi，双击安装。第二步：安装完成后使用cmd进入安装目录执行node -v可查看版本号。第三步：执行npm install -g grunt-cli 安装grunt，安装后执行grunt -version查看是否安装成功配置运行第一步：进入es安装目录下的config目录，修改elasticsearch.yml文件，在文件末尾加入以下代码。1234http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;node.master: truenode.data: true然后去掉network.host: 192.168.0.1的注释并改为network.host: 0.0.0.0，去掉cluster.name；node.name；http.port的注释（也就是去掉#）(!!!如果出现bat闪退的现象，注释就别去掉)第二步：双击elasticsearch.bat重启es第三步：在https://github.com/mobz/elasticsearch-head中下载head插件，选择下载zip第四步：解压到指定文件夹下，D:\environment\elasticsearch-head-master 进入该文件夹，修改D:\environment\elasticsearch-head-master\Gruntfile.js 在对应的位置加上hostname:’*’、第五步：在D:\environment\elasticsearch-head-master 下执行npm install 安装完成后执行grunt server 或者npm run start 运行head插件，如果不成功重新安装grunt。成功如下安装ik分词器什么事ik分词器IK Analyzer是一个开源的，基于java语言开发的轻量级的中文分词工具包。从2006年12月推出1.0版开始， IKAnalyzer已经推出了3个大版本。最初，它是以开源项目Luence为应用主体的，结合词典分词和文法分析算法的中文分词组件。新版本的IK Analyzer 3.0则发展为面向Java的公用分词组件，独立于Lucene项目，同时提供了对Lucene的默认优化实现。下载地址：https://github.com/medcl/elasticsearch-analysis-ik/releases、无需安装，解压即可使用,将其改名为ik，并且复制到elasticsearch的解压目录：然后重启elasticsearch。拓展词和停用词拓展词和停用词文件：总结以上就是elasticsearch的简介和安装，最终效果图如下：思考现在我们已经成功安装了es，可是遇到了es里面没有任何数据的问题，而爬虫爬的数据都在mysql中，如何把mysql数据同步到es，我将在下章博客告诉大家，敬请期待！]]></content>
      <categories>
        <category>搜索引擎</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[12.BIO、NIO、AIO的区别]]></title>
    <url>%2F12.BIO%E3%80%81NIO%E3%80%81AIO%E7%9A%84%E5%8C%BA%E5%88%AB.html</url>
    <content type="text"><![CDATA[同步IO和异步IOIO操作主要分为两个步骤，发起IO请求和实际IO操作，同步IO和异步IO的区别就在于第二个步骤是否阻塞。同步IO：实际IO操作阻塞请求进程，即请求进程需要等待或者轮询查看IO操作是否就绪。异步IO：实际IO操作并不阻塞请求进程，而是由操作系统来进行实际IO操作并将结果返回。阻塞IO和非阻塞IO区别在于发起IO请求是否阻阻塞IO：若发起IO请求后请求线程一直等待实际IO操作完成，则为阻塞IO。非阻塞：发送IO请求后线程返回而不会一直等待。BIO、NIO和AIOBIO：同步非阻塞式IO，服务器实现模式为一个连接一个线程，即客户端有链接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。NIO：同步非阻塞IO，服务器实现模式为一个请求一个线程，即客户端发送的连接请求都会注册到多路复用路上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。AIO：异步非阻塞IO，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由操作系统先完成IO操作后再通知服务器应用来启动线程进行处理。应用场景BIO适用于连接数目比较小且固定的架构，该方式对服务器资源要求比较高，JDK1.4以前的唯一选择。NIO适用于连接数目多且连接比较短(轻操作)的架构，如聊天服务器，编程复杂，JDK1.4开始支持，比如在Netty框架中使用。AIO适用于连接数目多且连接比较长(重操作)的架构，如相册服务器，充分调用操作系统参与并发操作，编程复杂，JDK1.7开始支持]]></content>
      <categories>
        <category>随记</category>
      </categories>
      <tags>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[14.如何将长URL转换成短URL]]></title>
    <url>%2F14.%E5%A6%82%E4%BD%95%E5%B0%86%E9%95%BFURL%E8%BD%AC%E6%8D%A2%E6%88%90%E7%9F%ADURL.html</url>
    <content type="text"><![CDATA[什么是短URL？ 顾名思义，就是将长网址缩短到一个很短的网址，用户访问这个短网址可以重定向到原本的长网址，这个可以达到易于记忆、转换的目的，常用于字数限制的微博、二维码等等场景。短URL的好处节省网址长度，便于社交文化传播，让URL更短小，传播更方便。短网址在我们项目里可以很好的对开放以及URL进行管理，有一部分网址可以会涵盖性、暴力、广告等信息。我们可以通过用户的季报，完全让这个连接将不出现在我们的应用中。方便后台跟踪点击量、地域分布等用户统计。我们可以对一系列的网址进行流量，点击等统计，挖掘出大多数用户的关注点。规避关键词、域名屏蔽手段、隐藏真实地址，适合做付费推广链接。 在微博或者短信场景，字数限制，用短网址能腾出很多空间。如何生成短地址URL利用发号器，初始值为0，对于每个短连接生成的请求，都递增发号器的值，再将值转换成62进制。将短连接服务器名与发号器的62进制值进行字符串连接，即为短链接的URL，例如：t.cnsBc。 生成短连接后，需要存储短链接和长链接的映射关系，即sBc-URL,浏览器访问短链接服务器时，根据url取到原始连接，进行302重定向。可以使用Redis或Memcache。跳转用301还是302？301是永久重定向，302是临时重定向。短地址一经生成就不会变化，所以用301是符合http语义的，同时对服务器压力也会减少。但是使用了301，我们就无法统计到短连接被点击的次数了。而这个点击次数就是一个非常有意思的大数据分析数据也，能够分析出的东西非常多，虽然302增加服务器压力，但依旧是个更好的选择。]]></content>
      <categories>
        <category>随记</category>
      </categories>
      <tags>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[13.HTTP长连接和短链接]]></title>
    <url>%2F13.HTTP%E9%95%BF%E8%BF%9E%E6%8E%A5%E5%92%8C%E7%9F%AD%E8%BF%9E%E6%8E%A5.html</url>
    <content type="text"><![CDATA[HTTP协议与TCP/IP的关系HTTP的长连接和短连接本质上就是TCP的长连接和短连接。HTTP属于应用层协议，在传输层使用TCP协议，在网络层使用IP协议。IP协议主要解决网络路由和寻址问题，TCP协议主要解决如何在IP层之上可靠的传递数据包，使在网络上的另一端收到发端发出的所有包，并且顺序与发出顺序一致。TCP有可靠，面向连接的特点。如何理解HTTP协议是无状态的指的是协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态。也就是说，打开一个服务器上的网页和你之前打开这个服务器上的网页之间没有任何关联。什么是长连接、短连接？在HTTP/1.0中默认是用的是短连接。也就是说，游览器和服务器每进行一次HTTP操作，就建立一次连接，但任务结束就中断连接。如果客户端浏览器访问的某个HTML或者其他Web页中包含其他的Web资源，如JavaScript文件、图像文件、CSS文件等；当浏览器每遇到这样一个Web资源，就会建立一个HTTP会话。**HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头有加入这行代码：1Connection:keep-alive当一个页面打开完成后，客户端和服务器之间用于传输HTTP数据的 TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接。操作过程短连接：建立连接——数据传输——关闭连接…建立连接——数据传输——关闭连接长连接建立连接——数据传输…(保持连接)…数据传输——关闭连接优缺点长连接优点省去较多的TCP建立和关闭的操作，减少浪费，节约时间。缺点存活功能的探测时间太长，Client与Server之间的连接如果不一直关闭的话，会存在一个问题，随着客户端连接越来越多，server早晚有扛不住的时候。短连接优点对于服务器管理较为简单，存在的连接都是有用的连接，不需要额外的控制手段。缺点如果客户端请求频繁，将在TCP的建立和关闭操作上浪费时间和带宽。使用场景长连接用于操作频繁，点对点的通讯，而且连接数不能太多情况。比如TCP连接的三次握手。数据库的连接。短连接像WEB网站的http服务一般都用短连接，因此会消耗一定的资源。]]></content>
      <categories>
        <category>随记</category>
      </categories>
      <tags>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20.高性能MySQL(六)]]></title>
    <url>%2F20.%E9%AB%98%E6%80%A7%E8%83%BDMySQL(%E5%85%AD).html</url>
    <content type="text"><![CDATA[一、为什么查询速度会慢如果把查询看作是是一个任务，那么它由一系列子任务组成，每个子任务都会消耗一定的时间。如果要优化查询，实际上要优化其子任务，要么消除其中一些子任务，要么减少子任务的执行次数。通常来说，查询的生命周期大致可以按照顺序来看：从客户端，到服务器，然后在服务器时进行解析，生成执行计划，执行，并返回结果给客户端。”执行”是最重要的阶段，包括了大量为检索数据到存储引擎的调用以及调用后的数据处理，包括排序、分组等。完成这些任务的时候，查询需要在不同的地方花费时间，包括网络、CPU计算，生成统计信息和执行计划，锁等待，还有内存操作、CPU操作和内存不足时导致I/O操作消耗时间。二、慢查询基础：优化数据访问大部分查询性能低下的原因是访问的数据太多，我们可以用下面两个步骤来分析：确认应用程序是否在检索大量超过需要的数据。通常访问太多行，但有时候也可能是访问太多列。确认MySQL服务层是否在分析大量超过需要的数据行。是否向数据库请求了不需要的数据查询不需要的记录例如某新闻网站中取出100条记录，但是页面只显示10条，他们认为MySQL会执行查询，并返回10条数据，然后停止查询，实际情况是MySQL会查出所有结果集，客户端接收全部结果集，然后抛弃大部分数据，最有效的解决方法是在查询后面加LIMIT。多表联查时返回全部列例如：1mysql&gt; SELECT * from a INNER JOIN B on a.id = b.pid总是取出全部列尽量避免使用SELECT *。重复查询相同的数据例如在用户评论的地方需要查询用户头像的URL，那么用户多次评论时候，可能会反复查询这个数据，比较好的方案是当初次查询时把数据缓存起来，需要的时候从缓存中取出。MySQL是否在扫描额外的记录在确定查询只返回需要的数据后，接下来应该看看查询为了返回结果是否扫描了过多的数据。对于MySQL，衡量查询的三个指标如下：响应时间扫描行数返回行数三个指标都会记录到MySQL的慢日志中，所以查询慢日志是找出扫描行数过多的查询的好办法。响应时间是两个部分之和：服务时间和等待时间。服务时间是指数据库处理这个查询真正花了多长时间。排队时间是指服务器因为等待某些资源而没有真正执行查询的时间————可能是等I/O操作完成，也可能是等待行锁。当你看到一个查询的响应时间时，先问问自己，这个时间是否在一个合理的值。扫描的行数和返回的行数分析查询时，查看该查询扫描的行数是非常有帮助的，理想情况下扫描的行数和返回的行数应该是相同的。实际情况这种情况并不多，在关联查询的时候，通常扫描和返回行数比位1:1-10：1之间。扫描的行数和访问类型在EXPLAIN语句中type列反应了方位类型。访问类型有很多种，从全表扫描到索引扫描、范围扫描、唯一索引查询、常数引用等。速度从慢到快，扫描的行数也是从大到小。如果查询没有办法找到合适的访问类型，那么解决的最好办法通常就是增加一个合适的索引。一般MySQL能够使用如下三种方式应用WHERE条件，从好到坏依次为：在索引中使用WHERE条件来过滤不匹配的记录。这是在存储引擎层完成的。使用索引覆盖扫描来返回记录。直接从索引中过滤不需要的记录并返回命中的结果。从数据表中返回数据，然后过滤不满足条件的记录。这在MySQL服务器层完成，需要先从数据表读出记录然后过滤。如果发现查询需要扫描大量的数据但是只返回少数的行，那么通常可以尝试下面的技巧去优化它：使用索引覆盖扫描，把所有需要用的列都放到索引中，这样存储引擎无须回表获取对于行就额可以返回结果了。改变库表结构。例如使用单独的汇总表。重写这个复杂的查询，让MySQL优化器能够以更优化的方式执行这个查询。三、重构查询的方式一个复杂查询还是多个简单查询在传统实现中，总是强调数据层完成尽可能多的工作，但是有时候，将一个大查询分解为多个小查询是很有比较的。不弱在应用设计的时候，如果一个查询能够胜任还写成多个独立查询是不明智的。例如，我们看到有些应用对一个数据表做10次独立的查询来返回10行数据，每个查询返回一条结果，查询10次！切分查询对于一个大查询我们需要”分而治之”，将大查询切分成小查询，每个查询功能完全一样，只完成一小部分，每次只返回一小部分的查询结果。删除旧数据就是很好的例子，如果使用一个大的语句一次完成的话，则需要一次锁住很多数据，沾满整个事务日志、耗尽系统资源，阻塞很多小的但很重要的查询。例如：我们每个月运行一次下面的查询：1mysql&gt; DELETE FROM messages WHERE created &lt; DATE_SUB(NOW()，INTERVAL 3 MONTH);那么可以用下面的方法来完成同样的工作：123456rows_affected = 0do&#123; rows_affected = do_quert( &quot;DELETE FROM messages WHERE created &lt; DATE_SUB(NOW(),INTERVAL 3 MONTH) LIMIT 10000&quot;) ) where rows_affected &gt; 0&#125;一次删除一万行是一个比较搞笑而且对服务器影响最小的做法。同事注意的是，每次删除数据之后，都暂停一会再做下一次删除，这样可以将服务器原来一次的压力分散到一个很长的时间段，就可以大大的降低对服务器的影响，还可以大大减少删除时对锁的持有时间。分解关联查询很多应用都会对关联查询进行分解，可以对每一个表进行一次单表查询，然后将结果在应用中关联。例如，下面这个查询：1234mysql&gt; SELECT * FROM tag -&gt; JOIN tag_post ON tag_post.tag_id = tag.id -&gt; JOIN post ON tag_post.post_id = post.id -&gt; WHERE tag.tag = &apos;mysql&apos;;可以分解为：1234mysql&gt;SELECT * FROM tag WHERE tag=&apos;mysql&apos;mysql&gt;SELECT * FROM tag_post WHERE tag_id = 1234;mysql&gt;SELECT * FROM post WHERE post.id in (123,456,567,9098,8904);优点：让缓存效率更高，可以更方便地缓存单表查询对应的结果对象，比如上面查询的tag已经被缓存了，那么就可以跳过第一个查询，拆分后，如果某个表很少改变，那么基于该表的查询就可以重复利用查询缓存结果了。执行单个查询可以减少锁竞争。做应用层管理，可以更容易对数据库进行拆分，更容易做到高性能和可扩展。查询本身效率会提示，上面例子中，使用IN()代替关联查询，可以让MySQL按照ID顺序进行查询。减少冗余记录查询，意味着某条记录应用只需要查询一次，而在数据库关联，则可能需要重复地访问一部分数据。四、查询执行的基础当向MySQL发送一个请求时，MySQL到底做了什么:客户端发送一条查询给服务器。服务器先检查查询缓存，如果命中了缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段。服务端进行SQL解析，预处理，再由优化器生成对应的执行计划。MySQL根据优化器生成的执行计划，调用存储引擎的API来执行查询。将结果返回给客户端。MySQL客户端/服务器通信协议。协议是”半双工”的，意味着在任何时刻，要么是服务器向客户端发送数据，要么是客户端向服务器发送数据，两个动作不能同时发生。查询状态对于一个MySQL连接，或者说一个线程，任何时刻都有一个状态，该状态表示了MySQL当前在什么。使用SOW FULL PROCESSLIST命令可以查看，主要有下面几种状态：Sleep:线程正在等待客户端发送新的请求。Query:线程正在执行查询或者正在将结果返回给客户端。Locked:在MySQL服务器层，该线程正在等待表锁Analyzing and statistics：线程正在收集存储引擎的统计信息，并生成查询的执行计划。Copying to tmp table[on disk]:线程正在执行查询，并且将其结果集都复制到一个临时表，这种状态一般要么是在做GROUP BY操作，要么是文件排序操作，或者是UNION操作。如果后面还有”on disk”标记，那表示MySQL正在将一个内存临时表放到磁盘上。Sorting result:线程正在对结果集进行排序。Sending data:线程可能在多个状态之间传送数据，或者在生成接过戒，或者在向he护短返回数据。查询缓存在解析一个查询语句之前，如果查询缓存是打开的，那么MySQL会优先检查这个查询是否命中查询缓存中的数据。这个检查是通过对一个大小写敏感的哈希查找实现的，即使有一个字节不同，都不会匹配缓存结果。如果命中了缓存，在返回查询结果之前MySQL会检查一次用户权限，如果没有问题，MySQL会跳过其他所有阶段，直接从缓存中拿到结果返回到客户端。查询优化处理查询的下一步是将一个SQL转换成一个执行计划，MySQL再依照这个执行计划和存储引擎进行交互。这包括多个子阶段：解析SQL、预处理、优化SQL执行计划，这个过程中出现任何错误都会终止查询。语法解析器和预处理首先MySQL通过关键字将SQL语句进行解析，并生成一颗对应的”解析树”。使用MySQL语法规则验证和解析查询，比如验证是否使用错误的关键词，或者使用关键词的顺序是否正确，验证引号是否能前后正确匹配。预处理器则根据一些MySQL规则进一步检查解析树是否合法。例如检查数据表和数据列是否存储在，还会解析名字和别名，看它们是否有歧义。查询优化器作用是找出这其中最好的执行计划。有很多原因会导致MySQL优化器选择错误的执行计划：统计信息不准确。执行计划中的成本估算不等同于实际执行的成本。MySQL的最优可能和你想的最优不一样。MySQL从不考虑其他并发执行的查询，这可能会影响到当前查询的速度。MySQL也并不是任何时候都是基于成本的优化。MySQL不会考虑不受其控制的操作的成本，例如执行存储过程或者用户自定义函数的成本。优化策略简单的分为两种，一种是静态优化，一种是动态优化。静态优化：可以直接对解析树进行分析，并完成优化。例如可以通过一些简单的点出变换将WHERE条件转成另一种等价形式。静态优化不依赖于特别的数值，例如WHERE中带入的一些常数。在第一次完成后就一直有效，既是使用不同参数执行查询页不会发生变化。动态优化：和查询的上下文有关，也可能和很多其他因素有关，例如WHERE条件中的取值，索引中条目对于的数据行数等。这需要在每次查询的时候重新评估。下面是一些MySQL能够处理的优化类型：重新定义关联表顺序将外连接转化成内连接使用等价变换规则优化COUNT()、MIN()和MAX()预估并转化为常数表达式覆盖扫描索引子查询优化提前终止查询等值传播列表IN()的比较数据和索引的统计信息因为服务器没有任何统计信息，索引MySQL查询优化器在生成查询的执行计划时，需要向存储引擎获取相应的统计信息。存储引擎则提供给优化器对于的统计信息。包括：每个表或者索引有多少个页面、每个表的索引的基数是多少、数据行和索引长度、索引的分布信息等。MySQL如何执关联查询总的来说，MySQL任务任何一个查询都是一次”关联”————并不仅仅是一个查询需要到两个表匹配才叫关联。MySQL执行策略很简单：对于任何关联都执行嵌套虚幻关联操作，即MySQL先在一个表中循环取出单条数据，然后再嵌套循环到下一个表中寻找匹配的行，依次下去，直到找到索引表中匹配的行为止。然后根据各个表匹配的行，返回查询中需要的各个列。关联查询优化器MySQL优化器最重要的一部分就是关联查询优化，它决定了多个表关联时的顺序。通常多表关联的时候，可以有多种不同的关联顺序来获得相同的执行结果。关联查询优化器则通过评估不同顺序时的成本来选择一个代价最小的关联顺序。排序优化两种排序算法两次排序算法(旧版本使用)：读取行指针和需要排序的字段，对其进行排序，然后再根据排序结果读取所需要的数据行。这里需要进行两次数据传输，即需要从数据表中读取两次数据，第二次读取数据的时候，因为是读取排序列进行排序后的所有记录，这会产生大量的随机I/O，所有两次数据传输的成本非常高。优点：在排序的时候存储尽可能少的数据，这就让”排序缓冲区”中可能容纳尽可能多的行数进行排序。单次传输排序：先读取查询所需要的所有列，然后再根据给定列进行排序，最后直接返回排序结果。缺点：如果返回的列非常多、非常大、会额外占用大量的空间，而这些列对排序操作本身来说没有任何作用。因为单条排序记录很大，所以可能会有更多的排序块需要合并。查询执行引擎在解析和优化阶段，MySQL将生产查询对于的执行计划，MySQL的查询执行引擎则根据这个执行计划来完成整个查询。返回结果给客户端最后一个阶段是将结果返回给客户端。如果查询可以被缓存，则会将MySQL在这个阶段也会将结果存放到查询缓存中。MySQL将结果集返回客户端是一个增量、逐步返回的过程。当开始生产第一条结果时，MySQL就可以开始向客户端逐步返回结果集了。服务端无需存储太多的结果。也就不会 因为要返回太多结果而消耗太多内存，另外，也让MySQL客户端第一时间获得返回的结果。五、MySQL查询优化器的局限性关联子查询MySQL会将相关的外层表压倒子查询中，它认为这样可以更高效地查询到数据行。UNION的限制无法限制调教从外层”下推”到内层，这使得原本能够限制部分返回的结果的条件应用到内层查询的优化上。例如：如果想将两个子查询结果联合起来，然后再取前20条记录，那么MySQL会将两个表都存放到同一个临时表中，然后再取出前20行记录：123456(SELECT first_name,last_nameFROM sakila.actor ORDER BY last_name) UNION ALL (SELECT first_name,last_name FROM sakila,customer ORDER BY last_name) LIMIT 20;这条查询将会把actor中的200条记录和customer表中的599条记录存放在一个临时表，然后再从临时表中取出前20条。可以通过在UNION的两个子查询中分别加上一个LIMIT 20来减少临时表中的数据：12345678910(SELECT first_name,last_nameFROM sakila.actor ORDER BY last_name LIMIT 20) UNION ALL (SELECT first_name,last_name FROM sakila,customer ORDER BY last_nameLIMIT 20) LIMIT 20;#### 索引合并优化&gt; 当WHERE子句中包含多个复杂条件的时候，MySQL能够访问单个表的多个索引以合并和交叉过滤的方式来定位需要查找的行。等值传递会带来一些意想不到的额外消耗。例如，有一个非常大的IN()列表，而MySQL优化器发现存在WHERE、ON或者USING的子句，将这个列表的值和另一个表的某个列相关联。并行执行MySQL无法利用多核特性来并行执行查询。哈希关联MySQL并不支持哈希关联，不过可以通过建立一个哈希索引来曲线的实现哈希关联。如果使用的是Memry存储引擎，则所有都是哈希索引，所以关联的时候也类似于哈希关联。松散索引扫描假如我们有如下索引(a,b)，有下面的查询：1mysql&gt;SELECT ... FROM tb1 WHERE b BETWEEN 2 AND 3;因为索引的前导字段是列a,但是在查询中只指定了字段b，MySQL无法使用这个索引，只能通过全表扫描找到匹配的行。最大值和最小值优化对于MIN()和MAX()查询，MySQL的优化做的并不好，例如：1mysql&gt; SELECT MIN(actor_id)FROM sakila.actor WHERE first_name = &apos;PENELOPE&apos;;因为咋first_name字段上并没有索引，因此MySQL读到第一个满足条件的记录的时候，就不是外貌需要找的最小值了。但是主键是严格按照actor_id字段的大小顺序排列的。但是MySQL这只会做全表扫描。一个曲线的优化方法是移除MIN(),然后使用LIMIT来将查询重写如下：12mysql&gt;SELECT actor_id FROM sakila.actor USER INDEX(PRIMARY) -&gt;WHERE first_name = &apos;PENELOPE&apos; LIMIT 1;六、优化特定类型的查询优化COUNT()查询COUNT()的作用：统计某个列的数量，要求列值是非空的。统计行数，当MySQL确认括号内的表达式不可能为空时，实际上就是在统计行数。关于MyISAM的神话一个容易产生的误解：MyISAM的COUNT()函数非常快，不过这是有前提条件的，即没有WHERE条件的COUNT(*)才非常快。当图片吗个抠带WHERE子句的结果集行数，可以是统计某个列值的数量时，MyISAM的COUNT()和其他存储引擎没有任何不同。简单的优化有时候可以通过使用MyISAM在COUNT(*)全表非常快的这个特性，来加速一些特定条件的COUNT()的查询。例如下面例子中，我们要获得所有ID大于5的城市：1mysql&gt;SELECT COUNT(*) FROM world.City WHERE ID &gt; 5;需要查询4097行数量。如果将条件反转下，先查询ID小于5的城市，然后用总城市数一减就能得到相同的结果，却能让扫描的行数减少到5行内。12mysql&gt;SELECT (SELECT COUNT(*) FROM world.City)-COUNT(*) -FROM world.City WHERE ID &lt;= 5;使用近似值有些业务场景并不需要完全精确的COUNT值，此时可以用近似值来代替，例如统计当前活跃用户数是多少，这个活跃用户数保存在缓存中，过期时间为30分钟，所以每隔30分钟需要重新计算放入缓存，因此这个活跃用户数本身就不是精确值。更复杂的优化除了前面的方法，在MySQL层面上还能做的就是索引覆盖扫描。如果这还不够，就需要考虑修改应用的架构，可以增加汇总表。优化关联查询确保ON或者USING子句中的列上有索引。在创建索引的时候就要考虑到关联的顺序。当表A和表B用列c关联的时候，如果优化器的关联顺序是B、A，那么就不需要在B表的对应列上建上索引。没有用到的索引只会带来额外的负担。确保任何的GROUP BY和ORDER BY中国的表达式只涉及到一个表中的列，这样MySQL才有可能使用索引来优化这个过程。当升级MySQL的时候需要注意：关联语法、运算符优先级等其他可能会发生变化的地方。优化子查询尽可能的使用关联查询来代替优化GROUP BY和DISTINCT很多情况下。MySQL优化器都会使用索引来优化，如果无法使用索引的时候，GROUP BY使用两种策略来完成：使用临时表或者文件排序来做分组。如果需要对关联查询做分组，并且按照查找表中的某个列进行分组，那么铜牛广场采用查找表的标识符列分组的效率会比其他列更高。例如下面的查询效率不会很好1234mysql&gt; SELECT actor.first_name,actor.last_name,COUNT(*) -&gt; FROM sakila.film_actor -&gt; INNER JOIN sakila.actor USING(actor_id) -&gt;GROUP BY actor.first_name,actor.last_name;如果查询按照下面的写法效率则会更高：1234mysql&gt; SELECT actor.first_name,actor.last_name,COUNT(*) -&gt; FROM sakila.film_actor -&gt; INNER JOIN sakila.actor USING(actor_id) -&gt;GROUP BY film_actor.actor_id;优化GROUP BY WITH ROLLUP分组查询的一个变种就是要求MySQL对返回的分组结果再做一次超级聚合，可以使用WITH ROLLUP子句来实现这种逻辑，但可能会不够优化。尽可能的将WITH ROLLUO功能转移到应用程序中处理。优化LIMIT分页。通常使用LIMIT加上偏移量的办法实现，同时加上合适的ORDER BY子句。如果有对应的索引，通常效率会不错。但是如果偏移量非常大的时候，例如LIMIT 1000,20这样的查询，这时候MySQL需要查询10029条记录然后只返回最后20条，前面10000条记录都会被抛弃，这样代价非常高。优化这种查询，要么是在页面中限制分页的数量，要么是优化大偏移量的性能。优化此类分页查询最简单的办法就是尽可能地使用索引覆盖扫描，而不是查询所有的列，然后根据需要做一次关联操作再返回所需的列。考虑如下查询：1mysql&gt; SELECT film_id,description FROM sakili.film ORDER BY title LIMIT 50,5;如果这个表非常大，那么这个查询最好改写成下面的样子：123456mysql&gt; SELECT film.film_id,film,description -&gt; FROM sakila.film -&gt; INNER JOIN( -&gt; SELECT film_id FROM sakila.film -&gt; ORDER BY title LIMIT 50,5 -&gt; ) AS lim USING(film_id);这里的”延迟关联”将大大提升查询效率它让MySQL扫描尽可能少的页面，获得要访问的记录然后再更加关联列回原表查询需要的所有列。有时候也可以将LIMIT查询转换为已知位置的查询，让MySQL通过范围扫描获得到对于的结果。例如，如果在一个位置列上有索引，并且预先计算出边界值，上面的查询就可以改为：12mysql&gt;SELECT film_id,description FROM sakila.film -&gt; WHERE postion BETWEEN 50 AND 54 ORDER BY postion;如果可以使用书签记录上次取数据的位置，那么下次就可以直接从该书签记录的位置开始扫描，这样就可以避免使用OFFSET。优化 SQL_CALC_FOUND_ROWS将具体页数转换成”下一页”按钮，假设每页显示20条记录，那么我们每次查询都用LIMIT返回21条记录并只显示20条，如果21条存在，那么我们就显示”下一页”按钮，否则就说明没有更多的数据，也就无须显示”下一页”按钮了。另一种方法就是获取并缓存较多的数据，例如，缓存1000条数据，然后每次分页都从这个缓存中获取，就可以让应用程序根据结果集的大小采取不同的策略，如果结果集少于1000，就可以在页面上显示所有的分页连接，因为数据都是在换乘站，所以这样的性能不会用问题。优化UNION查询MySQL总是创建并填充临时表的方式来执行UNION查询，因此很多优化策略在UNION查询中没法很好地使用。除非区呃呃是需要服务器消除重复的行，否则就一定要使用UNION ALL，这一点很重要。如果没有ALL关键字，MySQL会给临时表加上DISTINCT选项，这会导致对整个临时表的数据做唯一性检查。这样做的代价非常高。即使有ALL关键字，MySQL仍然会使用临时表存储结果。事实上，MySQL总是很结果放入临时表，然后再读出，再返回给客户端。静态查询分析Percona Toolkit中的pt-query-advisor能够解析查询日志、分析查询模式，然后给出所有可能存在潜在问题的查询，并给出足够详细的建议。使用用户自定义变量是一个用来存储内容的临时容器，在连接MySQL的整个过程中都存在。可以使用下面的SET和SELECT语句来定义它们：123mysql&gt; SET @one :=1;mysql&gt; SET @min_actor :=(SELECT MIN(actor_id) FROM sakila.actor)；mysql&gt; SET @last_week := CURRENT_DATE-INTERVAL 1 WEEK;然后可以再任何可以使用表达式的地方使用这些自定义变量：1mysql&gt; SELECT ...WHERE col &lt;= @last_week;下面情况无法使用自定义变量使用自定义变量的查询，无法使用查询缓存。不能在使用常量或者标识符的地方使用自定义变量，例如表名、列名和LIMIT子句中。用户自定义变量的生命周期是在一个链接中有效，所有不能用它们来做连接间的通信。如何使用连接池或者持久化连接，自定义变量可能让看起来毫无关系的代码发生交互。不能显式地声明自定义变量的类型。确定未定义变量的具体类型的时机在不同MySQL版本中也可能不一样。如果你希望变量是整数类型，那么最好在初始哈的时候就赋值为0，如果希望是浮点型则赋值为0.0，如果希望是字符串则赋值’’,用户自定义变量的类型在赋值的时候会改变。MySQL优化器在某些场景下可能会将这些变量优化掉，这可能会导致代码不按预想的方式运行。赋值的顺序和赋值的时间点并不总是固定的，这依赖于优化器的决定。赋值符号:=的优先级非常低，所有需要注意，赋值表达式应该使用明确的括号。使用未定义变量不会产生任何语法错误。使用场景查询运行时计算总数和平均值。模拟GROUP语句中的函数FIRST()和LAST()。对大数据做一些数据计算。计算一个大表的MD5散列值。编写一个样本处理函数，当样本中的数值超过某个边界值的时候将其变成0。模拟读/写游标。在SHOW语句的WHERE子句中加入变量值。七、案例学习使用MySQL构建一个队列表是一种取巧的做法。典型的模式是一个表包含多种类型的记录；未处理记录、已处理记录、正在处理记录等。一个或多个消费者线程在表中查找未处理的记录，然后声称正在处理，当处理完后，再将记录更新成已处状态。通常有两个原因认为这样的处理方式并不合适。第一 :随着队列表越来越大和索引深度的增加，找到未记录的速度会随之变慢，可以通过将队列表分成两个部分来解决这个问题，就是将已处理记录归档或者存放到历史表。这可以始终保证队列表很小。第二 :一般的处理过程分两步，先找到未处理记录然后加锁。找到记录会增加服务器压力，而加锁操作则会让各个消费者进程增加竞争，因为这是一个串行化的操作。基础原则：尽量少做事，可以的话不要做任何事情，除非不得已。否则不要使用轮询，因为这会增加负担。尽可能地完成需要做的事情。尽量使用UPDATE代替先SELECT FOR UPDATE再UPDATE的写法，因为事务提交的速度越快，持有的锁时间就越短，保证数据集足够小。某些查询是无法优化的；考虑使用不同的查询或者不同的策略去实现相同的目的。有时，最好的办法就是将任务队列从数据库中迁移出来。Redis就是一个很好的队列容器，也可以使用memcached来实现。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[18.高性能MySQL(四)]]></title>
    <url>%2F18.%E9%AB%98%E6%80%A7%E8%83%BDMySQL(%E5%9B%9B).html</url>
    <content type="text"><![CDATA[一、选择优化的数据类型基本原则：更小的通常更好尽可能选择更小的数据类型，通常更快，因为它们占用更少的磁盘、内存和CPU缓存简单就好简单的数据类型通常需要更少的CPU周期。例如，整型比字符操作代价更低，因为字符集和校对规则使字符串比整型复杂。尽量避免NULL尽可能指定为NOT NULL，因为包含NULL的列优化更难，使得索引、索引统计和值比较都更复杂。NULL值列会使用更多的存储空间，当可为NULL的列被索引时，每个索引记录都需要一个额外的字节。很多MySQL的数据类型可以存储相同类型的数据，只是存储的长度和范围不一样、允许的精度不同，活着需要的物理空间不同。例如：DATETIME和TIMESAMP列都可以存储相同类型的数据：时间和日起，精确到秒，然而TIMESTAMP只使用DATETIME一半的存储空间，并且会根据时区变化，具有自动更新能力。另一方面，TIMESTAMP允许的时间范围要小的多。整数类型有两种类型的数字：整数和实数。可以使用：TINYINT,SMALLINT,MEDIUMINT,INT,BIGINT,分别使用8，16，24，32，64位存储空间。有可选的UNSIGNED属性，表示不允许负值。可以使正数的上线提升一倍，例UNSUGNED可存储范围0255，TINYINT的存储范围-128127。可以为整数类型指定宽度，但是没有意义：它不会限制值的合法范围，只是规定了MySQL的一些交互工具用来显示字符的个数，对于存储和计算来说，INT(1)和INT(20)是一样的。实数类型是带有小数部分的数字。它们不只是为了存储小数部分，也可以使用DECIMAL存储比BIGINT还大的整数。DECIMAL类型用于存储精确的小数，在MySQL5.0以及更高版本中，服务器自身实现了DECIMAL的高精度运算，最多允许65个数字。有多种方法可以指定浮点列所需要的精度，这会使得MySQL悄悄的选择不同的数据类型，或者在存储时对值进行取舍。这些进度是非标准的，所有我们建议只指定数据类型，不指定精度。MySQL使用DOUBLE作为内部浮点计算的类型。因为需要额外的空间和计算开销，所有应该尽量只在对小数进行精确计算时才是用DECIMAL————例如存储财务数据。但在数量量比较大的时候，可以考虑使用BIGINT代替DECIMAL。将需要储存的货币单位更加小数的位数乘以相应的倍数即可。假设要存储财务数据精确到万分之一分，则可以把所有金额乘以一百万，然后将结果存储在BIGINT里。这样可以同时避免浮点存储计算不精确和DECIMAL精确计算代价高的问题。字符串类型VARCHARVARCHAR类型用于存储可変长字符串，是最常见的字符串类型。它比定长类型要节省空间，因为它仅适用必要的空间。需要使用1或2个额外字节记录字符串的长度：如果列的最大长度小于或等于255，则只使用1个字节表示，否则使用2个字节。由于行是变长的，在UPDATE时可能使行变得比原来更长，这就导致需要额外的工作。如果一个行占用的空间增长，并且页内没有更多的空间可以存储，在这种情况下，不同的存储引擎的处理方式是不一样的。例如，MyLSAM会将行拆成不同的片段存储，InnoDB则需要分裂页来使行可以放进页内。使用场景：字符串列的最大长度比平均长度大很多，列的更新很少，所以碎片不是问题。CHARCHAR类型是定长的，当存储CHAR值时，MySQL会删除索引的末尾空格。CHAR值会更加需要采用空格进行填充以方便比较。适合存储很短的字符串。例如存储密码的MD5值，索引值都接近一个长度。对于经常变更的数数据，CHAR也比VARCHAR更好，因为定长的CHAR类型不容易产生碎片。对于非常短的列，CHAR比VARCHAR在存储空间上，例如用CHAR(1)来存储只有Y和N的值，如果采用单字节字符集只需要一个字节，但是VARCHAR(1)缺需要两个字节，因为还有一个记录长度的额外字节。BLOB和TEXT类型BLOB和TEXT都是为存储很大的数据库而设计的字符串数据类型，分别采用二进制和字符方式存储。与其他类型不同，MySQL把每个BLOB和TEXT值当作一个独立的对象处理。存储引擎在存储时通常会做特殊处理。当BLOB和TEXT值太大时，InnoDB会使用专门的”外部”存储区域来进行存储。BLOB和TEXT家族之间仅有的不同是BLOE类型存储的是二进制数据，没有排序规则或字符集，而TEXT类型有字符集和排序规则。MySQL不能将BLOB和TEXT列全部长度的字符串进行索引，也不能使用这些鄋消除排序。使用枚举(ENUM)代替字符串类型有时候可以使用枚举列代替常用的字符串操作。枚举列可以把一些不重复的字符串存储成一个预定义的集合。MySQL在内部会将每个值在列表中的位置保存为整数。MySQL会在内部将每个值在列表中的位置保存为整数。尽量避免使用数字作为ENUM枚举常量，这种双重性很容易导致混乱。时期和时间类型可以使用许多类型来保存日期和时间值，例如YEAR和DATE。MySQL能存储的最小时间粒度为秒。但是MySQL可以使用为微妙级的粒度进行临时运算，我们会展示怎么绕开这种存储限制。DATETIME能保存最大范围的值，从1001年到9999年，精度为秒。它把日期和时间封装到格式为YYYYMMDDHHMMSS的整数中，与时区无关。TIMESTAMP保存了从1970年1月1日以来的描述，它和UNIX时间戳相同。只使用4个字节的存储空间，因此范围比DATETIME小得多：只能表示从1970年到2038年。显示地值也依赖于时区。存储值为0的TIMESTAMP在美国东部时区显示为”1969-12-31 19:00:00”，与格林尼治差5个小时。如果在多个时区存储或者访问数据，TIMESTAMP和DATETIME的行为将很不一样。前者提供的值与时区有关系，后者则保存文本表示的时间和日期。TIMESTAMP在默认情况下，如果插入时没有指定第一个TIMESTAMP列的值，MySQL则设置这个列的值为当前时间。在插入一行记录时，MySQL默认也会更新第一个TIMESTAMP列的值。默认为NOT NULL。除了特殊行为之外，通常也应该尽量使用TIMESTAMP,因此它比DATETINE空间效率更高。位数据类型BIT可以使用BIT列在一列中存储一个或多个true/false值。BIT(1)定义一个包含单个位的字段，BIT(2)存储2个位。依次类推，BIT列的最大长度是64个位。MySQL把BIT当做字符串类型，而不是数据类型。在数字上下文的场景中检测时，结果将是位字符串转换成的数字。对于大部分应用，最好避免使用这个类型。SET如果需要保存很多true/false值，可以考虑合并这些列到一个SET数据类型，它在MySQL内部是以一系列打包的位的集合来表示的。选择标识符整数类型标识列最好的选择，因为它们可很快并且使用AUTO_UINCREMENT。字符串类型如果可能，应该避免使用字符串类型作为标识符，因此它们很消耗空间，并且通常比数字类型慢。MyISAM默认对字符串使用压缩索引，这会导致查询慢得多。特殊类型数据某些类型的数据并不直接与内存类型一致，低于秒级精度的时间戳就是一个例子，另一个领子是IPV4。二、MySQL schema设计中的陷阱太多的列MySQL的存储引擎API工作时需要在服务器层和存储引擎层之间通过行缓冲格式拷贝数据，然后在服务器层将缓冲内容解码成各个列。从行缓冲中将编码过的列转换成行数据结构的操作代价是非常高的。太多的关联Mysql限制了每个关联操作最多只能有61张表，一个粗略的经验法则，如果希望查询执行得快速且并发性好，单个查询最好在12个表内做关联。全能的枚举注意防止过度使用枚举，在MySQL中，当需要在枚举列表中则增加一个新的国家时就要做一次ALTER TABLE。变相的枚举枚举列允许在列中存储一组定义值中的单个值，集合(SET)则允许在列中存储一组定义值中的一个或多个值。有时候这可能比较容易导致混乱。非此发明的NULL我们之前写了避免使用NULL的好处，并且建立尽可能地考虑替代方案，既是需要存储一个事实上的”空值”到表中，也不一定非得使用NULL。也许可以使用0。三、范式和反范式对于任何给定的数据通常都有很多种表示方法，从完全的范式化到完全的反范式化，以及两者的折中。在范式化的数据库中，每个事实数据都会出现且并且只出现一次。相反，在反范式化的数据库中，信息是冗余的，可能会存储在多个地方。三大范式1NF：无重复的列无重复的列，表中的每一列都是不可分割的数据基本项，不满足1NF的数据库不是关系型数据库。2NF：数据完全依赖于主键不能存在仅依赖于关键一部分的属性。3NF:属性不传递依赖于其它非主属性非转件列必须直接依赖于主键而不能传递依赖。即不能是：非主键A依赖于非主键B，非主键B依赖于主键。范式的优点和缺点优点范式化的更新操作通常比反范式化要快。当书记较好地范式化时，就只有很少或者没有重复数据，所有只需要修改更少的数据。范式化表通常更小，可以更好地放在内存里，所以只需操作会更快。很少有多余的数据意味着检索列表数据时更少需要DISTINCT或者GROUP BY语句。缺点通常需要关联，代价昂贵，也可能会使一些索引无效。例如范式化可能将列放在不同的表中，而这些列如果在一个表中本可以属于同一个索引。反范式的优点和缺点优点因为数据都在一张表中，可以很好的避免关联。如果不需要关联表，既是是没有索引的全表扫描，也比关联也要快的多，因为这样避免了随机I/O。单独的表也能使有效的索引策略。缺点有很多重复数据，会占用更多的内存，查询的时候会较多的使用DROUP BY或DISTINCT等耗时耗性能的关键字当修改更新数据时候，范式更灵活，反范式要修改全部的数据，且容易出错。混用范式化和反范式化。网上案列举个例子，要查询社区里面最新的10个帖子，如果按照范式设计，那么将关联两个表，一张是帖子表，另外一张是会员表，整个查询如下：SELECT TOP 10 N.帖子标题, U.会员昵称,N.会员ID FROM 帖子表 NJOIN 会员表 UON N.会员ID=U.会员IDORDER BY N.帖子ID DESC这样的查询将产生两个聚集索引扫描，一个在帖子表上，另外一个在会员表上，然后再进行嵌套循环，当数据不多的时候，这样的查询没有问题，当两张 表都是百万数量级的时候，问题出现了，这个查询动不动就是几百毫秒，甚至更慢，这样的查询效率根本不能满足点评网对于网页速度的要求（一般不能超过100 毫秒），怎么办，当然要反范式，在帖子表里面添加冗余字段——会员昵称，这样我们就可以通过下面的查询达到同样的目的：SELECT TOP 10 帖子标题, 会员昵称,会员ID FROM 帖子表 ORDER BY 帖子ID DESC这个查询只需要通过一个聚集索引扫描就可以得到（用到哪个索引还会跟ORDER BY后面的字段有关，例子中用的是Primary Key作为排序的字段，所以用到的是聚集索引扫描），将两个查询放在一起查看执行计划，就会发现，第一个查询开销占了92%，而第二个才8%，也就是说， 第二个查询比起第一个查询，效率上优化了10倍以上，成果显著啊。&gt; 于是，在点评网的这个阶段，大量的冗余字段出现在表的设计当中，主要集中在会员的昵称以及商户的名称上，一方面，这些信息在其他应用中频繁的用到，另外一方面，这两个信息所在的主表都是特别大的表，做关联查询消耗太大，而且容易造成资源的争夺。但是新的问题随之产生，每当一个会员去更新自己的昵称的时候，我们会执行一个存储过程，这个存储过程的目的就是去更新大量的会员昵称的冗余字 段，这些更新对于一个活跃会员来说，将是非常耗时的，因为需要更新的数据实在太多,而Web 2.0的精髓之一就是个性化，这样的设计对于个性化来说，有着不可调和的矛盾。于是，范式就像王者一样归来了。王者归来范式这个王者的归来，也是需要一定的时机的。时机1：网站规模的发展，已经造成反范式的副作用越来越大；时机2：公司规模的发展，已经有足够的财力建立更好的硬件平台；于是，我们选择了Memcached， 一个分布式的缓存系统，我们将会员信息以实体类的方式保存在Memcached里面，只要是可序列化的数据，经过装箱和拆箱，都可以保存到 Memcached中并随时可以快速的访问到这些对象，Memcached可以解决大量数据的缓存并保持多台Web Server得到的缓存数据是一致的，于是，前面那个例子可以通过这样的查询来进行SELECT TOP 10 帖子标题,会员ID FROM 帖子表 ORDER BY 帖子ID DESC然后再通过缓存系统封装的批量读取的方法得到这些会员的昵称，再显示到网页上，于是，访问速度快了，冗余数据没了，只要有一套完善的缓存管理机 制，问题就迎刃而解了。有时候，你还会因为这样的查询得到一些惊喜，因为会员ID是一个整型的字段，当你用一些非PK的字段作为排序的字段时，可以把会员 ID包含在其索引里面（SQL Server 2005新特性），这样在索引扫描后可以避免一次聚集索引扫描，再一次的提高了查询效率。相信对于这个问题的讨论是不会结束的，随着点评网的继续发展，新的问题必然还会产生，互联网就是要随需应变，到时候再来跟大家分享。四、缓存表和汇总表术语”缓存表”和”汇总表”没有标准的含义。我们用缓存表来表示存储那些可以比较简单从schema其他表获得数据的表。而汇总表保存的是使用GROUP BY语句聚合数据的表。如果要获得过去24小时准确的消息发送数量，可以以每小时汇总表为基础，把前23个完整的小时的统计表中的计数全部加起来，最后再加上开始阶段和结束阶段不完整的小时内的计数。物化视图许多数据管理系统都提供一个呗称作物化视图的功能，实际上是预先计算并且存储在磁盘上的表，可以通过各种各样的策略刷新和更新。MySQL并不原始支持物化视图，使用开源工具Flexviews。主要有下面部分组成：变更数据抓取功能，可以读取服务器的二进制日志并解析相关的行变更。一系列可以帮助创建和管理视图的定义的存储过程。一些可以应用变更到数据库中物化视图的工具。计数器表计数器表在Web应用中很常见，可以用这种表缓存一个用户的朋友数、文件下载次数等。创建一个表存储计数器通常是个好主意，这样可使计数器表小且快。假设有一个计数器表，只有一行数据，记录网站的点击次数：123mysql&gt; CREATE TABLE hit_counter( -&gt; cnt int unsigned not null -&gt;) ENGINE = InnoDB网站每次点击都会导致对计数器进行更新：1mysql&gt; UPDATE hit_counter SET cnt = cnt + 1;问题在于，对于任何想要更新这一行的事务来说，这条记录这条记录上都有一个全局的互斥锁。这会使得这些食物只能进行串行执行。要获得更高的并发更新性能，也可以将计数器保存在多行中，每次随机选择一行进行更新。1234mysql&gt; CREATE TABLE hit_counter( -&gt; slot tinyint unsigned not nill primary key. -&gt; cnt int unsigned not null -&gt;) ENGINE=InnoDB;冉家预先在这张表增加100行数据，现在选择一个随机的槽进行更新：1mysql&gt; UPDATE hit_counter set cnt = cnt + 1 WHERE slot = RAND() * 100;想要获得统计结果，需要使用下面这样的聚合查询：1mysql&gt; SELECT SUM(cnt) FROM hit_counter;五、加快ALTER TABLE操作的速度第一种方法：先在一台不提供服务的机器上执行ALTER TABLE操作，然后和听歌服务的主库进行切换第二种方法：”影子拷贝”，要求表结构可以创建一张和源表无关的新表，然后通过重命名和删表操作交换两张表。不是所有的ALTER TABLE操作都会引起表重建。例如有两种方法可以改变或者删除一个列的默认值(一种很快，一种很慢)。例如要修改电音的默认组里期限，从三天改到五天，下面是很慢的方式：12mysql&gt; ALTER TABLE sakila.film -&gt; MODIFY COLUMN rental_duration TINYINT(3) NOT NULL DEFAULT 5;另一种是通过ALTER COLUMN操作来改变列的默认值：12mysql&gt; ALTER TABLE sakila.filn -&gt; ALTER COLUMN rental_duration SET DEFAULT 5;这种语句会直接修改.frm文件而不涉及表数据。快速创建MyISAM索引操作步骤用需要的表结构创建一张表，但是不包括索引。载入数据到表以构建.MYD文件。按照需要的结构创建另外一张空表。这次要包含索引。这会创建需要的.frm和.MYI文件。获取读锁并刷新表重命名第二张表的.frm和.MYI文件，让MySQL认为是第一张表的文件。释放读锁使用REPAIR TABLE 来重建表的索引，该操作会通过排序来构建索引索引，包括唯一索引。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[19.高性能MySQL(五)]]></title>
    <url>%2F19.%E9%AB%98%E6%80%A7%E8%83%BDMySQL(%E4%BA%94).html</url>
    <content type="text"><![CDATA[一、索引基础在MySQL中中，存储引擎用类似的方法使用索引，现在索引中找到对应值，然后根据匹配的索引记录找到对应的数据行，索引可以包含一个或多个列的值，如果索引包含多个列，那么列的顺序也十分重要，因为MySQL只能高效地使用索引的最左前缀列。创建一个包含两个列的索引，和创建两个只包含一列的索引是大不相同的。索引类型在MySQL中，索引是在存储引擎层而不是服务器层实现的。所以没有统一标准：不同存储引擎的索引的工作方式并不一样，也不是索引的存储引擎都支持所以类型的索引。既是多个存储引擎支持同一种类型的索引，其底层实现也可能不同。B-Tree索引如果没特别指明类型，一般默认是B-Tree类型。它使用B-Tree数据结构来存储数据，大多数MySQL存储引擎都支持这种索引。Archive引擎是个例外。不过底层的存储引擎也可能使用不同的存储结果，例如NDB集群存储引擎内部实际上使用了T-Tree结构存储这种索引；InnoDB则使用的是B+Tree。存储引擎以不同的方式使用B-Tree索引，性能也各有不同，各有优劣。例如，MyISAM使用前缀压缩技术使得索引更小，但InnoDB按照原书记格式进行存储。再如MyISAM索引通过书记的物理位置引用被索引的行，而InnoDB则根据主键引用被索引的行。B-Tree通常意味着所有的值都是按顺序存储的，并且每一个叶子页到根的距离相同。能够加快访问数据的速度，因为存储引擎不需要进行全表扫描来获取需要的数据，取而代之的是从索引的根节点开始搜索。根节点的槽中存放了指向子节点的指针，存储引擎根据这些指针向下层查找。通过比较节点页的值和要查找的值可以找到合适的指针进入下层子节点，这个鞋指针实际上定义了子节点页中值的上限和下限。最终存储引擎要么是找到对应的值，要么该记录不存在。叶子节点比较特别，它们的指针指向的是被索引的数据，而不是其他的节点页。B-Tree对索引列是顺序组织存储的，所有很适合查找范围数据。可以使用B-Tree索引的查询类型。B-Tree索引适用于全键值、键值范围或键前缀查找。其中键前缀查找只适用于根据最左前缀的查找，前面所述的索引对如下类型的查询有效。全值匹配，指的是和索引中的所有列进行匹配。匹配最左前缀，只是索引的第一列。匹配列前缀，只匹配某一列的值的开头部分，这里也只使用索引的第一列。匹配范围值，可以用于查找某一范围之间的数据，这里也只使用索引的第一列。精确匹配某一列并范围匹配另外一列。即第一列全匹配，第二列范围匹配。只范围索引的查询，即查询只需要访问索引，而无须访问数据库。因为索引树的节点是有序的，所以除了按值查找伊娃，索引还可以用于查询中的ORDER BY操作。使用限制：如果不是按照索引的最左列开始查找，则无法使用索引。不能跳过索引中的列，如果不指定名，则MySQL只能使用索引的第一列。如果查询中有某个列的范围查询，则其右边的列都无法使用索引优化查找。哈希索引基于哈希表实现，只有精确匹配索引所有列的查询才有效。对于每一行数据，存储引擎都会对索引的索引列计算出一个哈希码，是一个比较小的值，并且不同键值的行计算出的哈希码也不一样。哈希索引将所有的哈希码存储在索引中，同时在哈希表保存指向每个数据行的指针。因为索引自身只需要存储对应的哈希值，所有索引结构非常紧凑，这也让哈希索引查找的速度非常快。使用限制：哈希索引只包含哈希值和行指针，而不存储字段值，索引不能使用索引中的值来避免读取行。哈希索引数据并不是按照索引值顺序存储，所以也就无法用于排序。哈希索引也不支持部分索引列匹配查找，因为哈希索引始终是使用索引列的全部内容来计算哈希值的。例入，在数据列(A,B)上建立哈希索引，如果查询只有数据列A，则无法使用该索引。哈希索引只支持等值比较查询，包括=、IN()、&lt;=&gt;。不支持任何范围查询，例入WHERE price &gt; 100。访问哈希索引的数据非常快，除非有很多哈希冲突(不同的索引列值却有相同的哈希值)。当发生哈希冲突的试试，存储引擎必须遍历链表中所有的行指针，逐行进行比较，知道找到索引符合条件的行。如果哈希冲突很多的话，一些索引维护操作的代价也会很高。例如，在摸弍选择性很低(哈希冲突很多)的列上建立哈希索引，那么当从表中删除行时，存储引擎需要遍历对应哈希值的链表中的每一很纠结昂，找到并删除对应行的引用，冲突越多，代价越大。因为这些限制，哈希索引只适合某些特定场合。而一旦适合哈希索引，则它带来的性能提升非常显著。InnoDB引擎有个特殊的功能叫做”自适应哈希索引”。当InnoDB注意到某些索引值被使用的非常频繁时，它会在内存中基于B-Tree索引上再创建一个哈希索引，这样就让B-tree也有了哈希索引的一些优点，比如快速的哈希查找。这是个完全自动、内部的行为，用户无法控制或者配置，只能开启和关闭。创建自定义哈希索引。如果存储引擎不支持哈希索引，则可以模拟像InnoDB一样创建哈希索引，这可以享受一些哈希索引的便利，例如只需要很小的索引就可以为超长的键创建索引。思路很简单：在B-Tree基础上创建一个伪哈希索引。这还真正的哈希索引不是一回事。因为还是使用B-Tree进行查找，但是它使用哈希值而不是键本身进行索引查找。只需要在查询的WHERE子句手动指定使用哈希函数。唯一缺陷就是需要维护哈希值。可以手动维护，也可以使用触发器实现。空间数据索引(R-Tree)MyISAM表支持空间索引，可以用作地理数据存储。和B-Tree索引不同，这类索引无须前缀查询，空间索引会从索引维护来索引数据。查询时，可以有效地使用任意纬度来组合查询，必须使用MySQL的GIS相关函数如MBRCONTAINS()等来维护数据。MySQL的GIS支持并不完善，所以大部分人都不会使用这个特性。全文索引一种特殊类型索引。它查找的是文本中的关键词，而不是直接比较索引中的值，更类似于搜索引擎做的事情，也不是简单的WHERE条件匹配。在相同的列上同时创建全文索引和基于值的B-Tree索引不会有冲突，全文索引适用于MATCH AGAINST操作，而不是普通的WHERE条件操作。二、索引的优点能够让服务器快速定位到表的指定位置，但这不是唯一的作用，根据创建索引的数据结构不同，索引也有一些其他的附加作用。常见的B-Tree索引，按照顺序存储数据，所以MySQL可以用来做ORDER BY和GROUP BY操作。因为数据是有序的，所以B-Tree也就会将相关的列值都存储在一起。最后，因为索引中存储了实际的列值，所以某些查询只是用索引就能完成全部查询。据此，总结下来有如下三个优点：索引大大减少了服务器需要扫描的数据量。索引可以帮助服务器避免排序和临时表。索引可以将随机I/O变成顺序I/O。三、高性能的索引策略独立的列如果查询的列不是独立的，则MySQL就不会使用索引。索引列不能是表达式的一部分，也不能是函数的参数。例如下面这个查询无法使用actor_id列的索引：1mysql&gt;SELECT actor_id FROM sakila.actor WHERE actor_id + 1 = 5;下面是另一个常见的错误：1mysql&gt;SELECT...WHERE TO_DAYS(CURRENT_DATE) - TO_DAYS(date_col) &lt;= 10;前缀索引和索引选择性有时候索引很长的字符串，会让索引变得很大很慢，一个策略就是之前提到过的模拟哈希索引。但有时候这样做还不够，还可以做些什么？通常可以索引开始的部分字符串，可以大大节约索引空间，从而提高索引效率，但是这样会降低索引的选择性。选择性是指，不重复的索引值和数据表的记录总数(#T)的比值，范围从1/#T到1之间。索引的选择性越高，可以让MySQL在查找时过滤掉更多的行。唯一索引的选择性是1，这是最好的索引选择性，性能也是最好的。对于BLOB、TEXT或者很长的VARCHAR类型的列，必须使用前缀索引，因为MySQL不允许索引这些列的完整长度。诀窍在于要选择足够长的的前缀以保证较高的选择性，同时又不能太长。前缀应该足够长，以使得前缀索引的选择性接近于索引整个列。为了解决前缀的合适长度，需要找到最常见的值的列表，然后额最常见的前缀列表进行比较。下面演示下如何创建前缀索引：1mysql&gt;ALTER TABLE sakila.city_demo ADD KEY(city(7));前缀索引是一种能够使索引更小、更快的有效方法，但是也有其缺点：MySQL无法使用前缀索引做ORDER BY和GROUP BY，也无法使用前缀索引做覆盖扫描。多列索引很多人对多列索引的理解都不够。一个常见的错误就是，每个列创建独立的索引，或者按照错误的顺序创建多列索引。在多条列上建立独立的单列索引大部分情况下都不能提高MySQL的查询性能。在5.9和更新版本中引入了一种叫”索引合并”的策略，一定程度上可以使用表上的多个单列索引来定位指定的行。在MySQL5.0和更新的版本中，查询能够同时使用这两个单列索引进行扫描，并将结果进行何必。有三个变种：OR条件的联合(union)，AND条件的相交(intersection)，组合前两种情况的联合即相交。1mysql&gt;EXPLAIN SELECT film_id,actor_id FROM sakila,film_actor -&gt;WEHRE actor_id = 1 OR film_id = 1\G索引合并才做有时候是一种优化的结果，但实际上更多时候说明了表上的索引建得很糟糕：当出现服务器对多个索引索引做相交操作，通常意味着需要一个包含所有相关列的索引，而不是多个独立的单列索引。当服务器需要对多个索引做联合操作时，通常需要耗费大量CPU和内存资源在算法的缓存、排序和合并操作上。特别是当其中有些索引的选择性不高，需要合并扫描返回大量数据的时候。更重要的是，优化器不会把这些计算到”查询成本中”，优化器只关心随机的页面读取。这会使得查询的成本被低估，导致执行该计划还不如走全表扫描。这样不但会消耗更多的CPU和内存资源，还可能会影响查询的并发性。如果在EXPALIN看到有索引合并，应该好好检查下查询和表的结构，看是不是已经是最优的。选择合适的索引列顺序正确的顺序依赖于使用该索引的查询、并且同事需要考虑如何更好地满足排序和分组的需要。在一个多列B-Tree索引中，索引列的顺序意味着索引首先按照最左列进行排序，其次是第二列。索引可以按照升序或者降序进行扫描，以满足精确复符合列顺序的ORDER BY、GROUP BY和DISTINCTZ等子句查询的需求。经验法则：将选择性高的列放在索引最全列。当不需要考虑排序或者分组，将选择性最高的列放在前面通常是很好的。这时候索引的作用只是用于优化WHERE条件的查找。聚簇索引不是一种单独的索引类型，而是一种数据存储方式。InnoDB的聚簇索引实际上在同一个结果中保存了B-Tree索引和数据行。当表有聚簇索引时，它的数据行实际上存放在索引的叶子页中。”聚簇”表示数据行和相邻的键值紧凑地存储在一起。因为无法同时把数据行存放在两个地方，所以一个表只能由一个聚簇索引。InnoDB通过主键聚集数据，被索引的列就是主键列。如果没有定义主键，InnoDB会选择一个唯一的非空索引代替。如果没有这样的索引，InnoDB会隐式定义一个主键来作文聚簇索引。优点：可以把相关数据保存在一起。例如实现电子邮箱时，可以根据用户ID来聚集数据，这样只需要从磁盘读取少数的数据也就能获取某个用户的全部邮件。如果没有使用聚簇索引，则每一封邮件都可能导致一次磁盘I/O,数据访问更快，聚簇索引将索引和数据保存在同一个B-Tree中，因此从聚簇索引这种获取数据通常比在非聚簇索引中查找要快。使用覆盖索引扫描的查询可以直接使用页节点中的主键值。缺点：聚簇数据最大限度地提高了I/O秘籍型应用的性能，但如果数据全部都放在内存中，则访问的顺序就没那么重要了，聚簇索引也就没什么优势了。插入速度严格依赖插入顺序。更新代价太高。会产生页分裂问题，导致表占用更多的磁盘空间。导致全表扫描变慢。二级索引比想象中更大。二级索引访问需要两次索引查找。二级索引叶子节点保存的不是指向行的物理位置的指针，而是行的主键值，存储引擎需要找到二级索引的叶子节点获得对应的主键值，然后根据这个值去聚簇索引找到对应的行。对于InnoDB，自适应哈希索引能够减少这样的重复工作。聚簇索引和非聚簇索引区别在InnoDB表中按主键顺序插入行尽量使用自增主键，保证数据行按照顺序插入，如果使用UUIP作为聚簇索引，会使聚簇索引的插入变得完全随机，不仅花费时间更长，索引占用空间更大，因为写入是乱序的，InnoDB不得不频繁地做页分裂操作，以便问新的行分配空间覆盖索引如果一个索引包含(或者说覆盖)所有需要查询的字段的值，我们称之为覆盖索引。优点索引条目通常远小于数据行大小，所以如果只需要读取索引，那MYSQL就会极大地减少数据访问量，这对缓存的负载非常重要，因为这种情况下响应时间大部分花费在数据拷贝数。对于I/O密集型的应用也有帮助，因为索引比数据更小，更容易全部放在内存中。因为索引是按照列值顺序存储的，索引对于I/O密集型的范围查询会比随机从磁盘中读取每一行数据的I/O要少得多。由于InnoDB的聚簇索引，覆盖索引对InnDB表非常有用。InnDB的二索引在叶子节点中保存了行的主键值，所以如果二级主键能够覆盖查询，则可以避免对主键索引的二次查询。不是所有类型的索引都可以成为覆盖索引。覆盖索引必须要存储索引列的值，而哈希索引、空间索引和全文索引等都不存储索引列的值。索引无法覆盖查询原因没有任何索引能够覆盖这个查询。因为查询从表中选择了所有的列，而没有任何索引覆盖了所有的列。MySQL不能再索引中执行LIKE操作。这是底层存储引擎API的限制。可以使用延迟关联解决上述问题，因为延迟了对列的访问，在查询第一阶段MySQL可以使用覆盖索引，在FROM子句的子查询中找到匹配的prod_id,然后 根据这些prod_id值在外层查询匹配获取需要的所有列值。1234567mysql&gt; EXPLAIN SELECT * -&gt; FROM products -&gt; JOIN( -&gt; SELECT prod_id -&gt; FROM products -&gt; WHERE actor=&apos;SEAN CARREY&apos; AND title LIKE &apos;%APOLLP%&apos; -&gt; )As t1 ON(t1.prod_id = products.pro_id)\G使用索引扫描来做排序MySQL有两种方式可以生产有序的结果:通过排序操作；或者按照索引顺序扫描，如果EXPLAIN出来的type列的值为”index”。扫描索引本身是很快的，因为只需要从一条索引记录移动到警戒着的下一条记录。但是如果索引不能覆盖查询所需的全部列，那就不得不每扫描一条索引记录就回标查询一股对应的行。MySQL可以使用同一个索引既满足排序，又用于查找行，如果可能，尽量同时满足这两种任务。只有当索引的列顺序和ORDER BY子句顺序完全以组织，并且所有列的排序方向都一样，MySQL才能够使用索引来对结果做排序。如果查询需要关联多张表，则只有当ORDER BY子句引用的字段全部为第一个表时，才能使用索引做排序。OEDER BY子句和查找型查询的限制是一样：需要满足索引的最左前缀的要求，否则MySQL都需要执行排序操作，而无法利用索引排序(当导出量为常量时，可以不用满足)。压缩(前缀压缩)索引MyISAM使用前缀压缩来减少索引的大小，从而让更多的索引可以放在内存中，这些情况下能极大地提高性能。默认只压缩字符串，但是通过参数设置也可以对整数进行压缩。MyISAM压缩每个索引块的方法是，先完全保存索引块中的第一个值，然后将其他值和第一个值进行比较得到相同前缀的字节数和剩余的不同后缀部分。例如，索引块的第一个值是”perform”，第二个值是”performance”，那么第二个压缩后的结果为”7,ance”。因为每个压缩前缀都依赖前面的值，MyISAM无法在索引块通过二分查找只能从头扫描，正序扫描还不错，不建议倒序扫描。冗余和重复索引重复索引是指在相同列上按照相同的顺序创建的相同类型的索引。应该避免这样创建索引，发现后也应该立即删除。冗余索引和重复索引有一些不同。如果创建了索引(A,B),再创建索引(A)就是冗余索引，因为这只是前一个索引的前缀索引。因此索引(A,B)也可以当做索引(A)来使用。但是如果创建索引(B,A),则不是冗余索引。大多数情况下都不需要冗余索引，应该尽量拓展已有的索引而不是创建新的索引。但有时候出于性能方面的考虑需要冗余索引，因为拓展已有的索引会导致其变得太大，从而影响其他使用该索引的查询的性能。例如，如果在索引列上有一个索引，现在需要额外增加一个很长的VARCHAR列来拓展该索引，那么性能可能会急剧下降。特别是有查询把这个索引当作覆盖索引。未使用的索引除了冗余索引和重复索引，可能还有还有一些服务器永远不用的索引。建议删除，通常有两个方法可以定位：在Percona Server或者MariaDB中先打开userstates服务器变量，然后让服务器正常运行一段时间，再通过查询INFORMATION_SCHEMA.INDEX_STATISTICS就是查到每个索引的使用频率使用Percona Toolkit中的pt-index-usage，该工具可以读取查询日志，并对日志中的每个查询进行EXPLAIN操作，然后打印出相关索引和查询的报告。索引和锁索引可以让查询锁定更少的行，虽然InnnoDB的行锁效率很高，内存也使用的少，但是锁定行的时候仍然会带来额外的开销；其次，锁定超过需要的行会增加锁争用并减少并发性。InnoDB只有在访问行的时候才会对其加锁，而索引能减少InooDB访问的行数，从而减少锁的数量。但这只有当InnoDB在存储引擎能够过滤掉索引不需要的行时才有效。如果索引无法过滤掉无效的行，那么在InnoDB检查到数据并返回给服务器以后，MySQL服务器才能应用WHERE子句，这时候已经无法避免行锁定了：InnoDB已经锁定这些行，到适当的时候才能释放。案例学习需求：设计一个在线约会网站，用户信息表有很多列，包括国家、地区、城市、性别、眼睛颜色等。网站必须支持上面这些特征的各种组合来搜索用户，还必须允许根据用户的最后在线时间、其他会员对用户的评分等对用户进行排序并对结果进行限制。支持多种过滤条件首先看看哪些列拥有很多不同的取值，哪些列在WHERE子句中出现得最频繁。在更多不同值的列上创建索引的选择性会更好。country列的选择性通常不高，但可能很多查询都会用到。sex列的选择性肯定很低，但也会在很多查询中用到。所以考虑到使用的频率，还是建议在创建不同组合索引的时候将(sex,country)列作为前缀。即使没有使用sex列也可以通过这个”诀窍”绕过：如果某个查询不限制性别，那么可以通过在查询条件中新增AND SEX IN(‘m’,’f’)来让MySQL选择该索引。age列总是放在索引最后面，我们总是尽可能让MySQL使用更多的索引列，因为查询只能使用索引的最左前缀，直到遇到一个范围条件列。避免多个范围条件假设我们有一个last_online列并希望通过下面的查询显示在过去几周上线过的用户：12345WHERE eye_color IN(&apos;brown&apos;,&apos;blue&apos;,&apos;hazel&apos;) AND hair_color IN(&apos;black&apos;,&apos;red&apos;,&apos;blonde&apos;,&apos;brown&apos;) AND sex IN(&apos;M&apos;,&apos;F&apos;) AND last_online &gt; DATE_SUB(NOW(),INTERVAL 7 DAY) AND age BETWEEN 18 AND 25这样查询有个问题：它有两个范围条件，last_online列和age列，MySQL可以使用last_online列索引或者age列索引，但无法同时使用他们。什么是范围查询？EXPLAIN很难区分MySQL是要查询范围值还是查询列表值，EXPLAIN会使用同样的词’rang’来描述这两种情况。12345678910111213141516mysql&gt;EXPLAIN SELECT actor_id FROM sakila.actor -&gt;WHERE actor_id &gt; 45\G*************** 1. row *************** id: 1 select_type：SIMPLE table：actor type：range 但是下面这条查询呢？mysql&gt;EXPLAIN SELECT actor_id FROM sakila.actor -&gt;WHERE actor_id IN(1，4，99)\G*************** 1. row *************** id: 1 select_type：SIMPLE table：actor type：range第一个是范围查询，第二个是多个等值查询，对于范围查询，MySQL无法再使用范围后的其他索引列了，但是对于”多个等值条件查询”则没有这个限制。优化排序限制用户能够翻页的数量使用延迟关联，通过使用覆盖索引查询返回需要主键，再根据这些主键关联原表获得所需要的行。例：1234nysql&gt;SELECT &lt;cols&gt; FROM profiles INNER JOIN( -&gt; SELECT &lt;primary key cols&gt; FROM profiles -&gt; WHERE x.sex = &apos;M&apos; ORDER BY rating LIMIT 100009,10 -&gt; ) AS x USING(&lt;primary key cols&gt;)五、维护索引和表找到并修复损坏的表对于MyISAM存储引擎，表损坏通常是系统崩溃导致的。损坏的索引会到导致查询返回错误的结果或者莫须有的主键冲突等问题,可以通过两个指令来解决问题：CHECK TABLE：检查表是否发生损坏。REPAIR TABLE：修复损坏的表。如果InooDB引擎的表出现了损坏，那一定是发生了严重的错误。需要立刻调查原因，最重要的是找出什么导致了损坏，而不是简简单单的修复。可以通过设置innodb_force_recovery参数进入InnoDB的强制恢复模式来修复数据，也可以使用开源的InnoDB数据恢复箱。更新索引统计信息MySQL的查询优化器会通过两个API来了解存储引擎的索引值的分布信息，以决定如何使用索引。records_in_rang()通过向存储引擎传入两个边界值获取在这个范围大概有多少条记录，对于某些存储引擎 ，该接口返回精确值，例如:MyISAM；对于另一些存储引擎则是一个估算值，例如:InnoDB.info(),该接口返回各种类型的数据，包括索引的基数。减少索引和数据的碎片B-Tree索引可能会碎片化，这会降低查询的效率。碎片化的索引可能会以很差或者无序的方式存储在磁盘上。表的数据也可能碎片化，常见的有三种类型：行碎片数据行被存储为多个地方的多个片段中，即使查询只从索引中访问一行记录，行碎片也会导致性能下降。行间碎片指逻辑上顺序的页，或者行在磁盘上不是顺畅存储的。行间碎片对诸如全表扫描和去除索引扫描之类的操作有很大的影响，因为这些操作原本能够从磁盘上顺序存储的数据中收益。剩余空间碎片指数据页中有大量的空余时间。这会导致服务器读取大量不需要的数据，从而造成浪费。对于MyISAM表，三种情况都有可能发生。但InnoDB不会出现短小的行碎片，会移动短小的行并重写到一个片段中。解决方法可以通过执行OPTIMIZE TABLE或者导出再导入1的方式来重新整理数据。对于MyISAM,可以通过排序算法重建索引的方式来消除碎片。对于新版InnoDB，新增了”在线”添加和删除索引的功能，可以通过先删除，然后再重新创建索引的方式来消除索引的碎片化。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[17.高性能MySQL(三)]]></title>
    <url>%2F17.%E9%AB%98%E6%80%A7%E8%83%BDMySQL(%E4%B8%89).html</url>
    <content type="text"><![CDATA[一、性能优化简介性能定义为完成某件任务所需要的时间度量，换句话说，性能即响应时间，通过任务和时间而不是资源来测量性能。数据库服务器的目的是执行SQL语句，所有它关注的任务是查询或者语句，如SELECT、UPDATE、DELETE等。数据库服务器的性能用查询的响应时间来度量，单位是每个查询花费的时间。如果你认为性能优化是降低CPU利用率，那么可以减少对资源的使用，那么打错特错，资源是用来消耗并用来工作的，所有有时候消耗更多的资源能够加快查询速度。如果把性能优化仅仅看成提升每秒查询量，这其实只是吞吐量优化。吞吐量替身可以看做性能优化的副产品。当优化时，应该把精力和时间都来测量响应时间花在哪里，对症下药的解决方案也就比较明了了。有两种比较常见的情况会导致不合适额测量：在错误的时间启动和停止测量测量的是聚合后的信息，而不是目标活动本身。例如，一个常见的错误哦是先查看慢查询，然后又去排查整个服务器的情况来判断问题在哪里。如果确认有慢查询，那么就应该测试慢查询，而不是测量整个服务器。测量的应该是慢查询的开始时间到结束时间，而不是查询之前或查询之后的时间。完成一项任务所需要的时间可以分成两个部分：执行时间和等待时间。如果要优化执行时间，最好的方法就是通过测量定位不同的子任务花费的时间，然后优化去掉一些子任务、降低子任务的执行频率或者提升子任务的效率。而优化任务的等待时间相对于复杂一些，因为等待有可能是由其他系统间接影响导致，任务之间也可能由于争用磁盘或者CPU自由而相互影响。通过性能剖析进行优化性能剖析是测量和分析时间花费在哪里的主要方法。一般有两个步骤：测量任务所花费的时间；然后对结果进行统计和排序，将重要的任务排到前面。性能剖析工具的方式基本相同，在任务开始时启动计时器，在任务结束时停止计时器，然后用结束时间减去启动时间得到响应时间。也有工具会记录任务的父任务。结果数据用来绘制调用关系图。实际讨论两种类型的性能剖析:基于执行时间的分析和基于等待的分析。基于执行时间的分析研究的是什么任务的执行时间最长，而基于等待的分析这是判断任务在什么地方被阻塞的时间最长。如果任务执行时间是因为消耗了太多的资源且大部分时间花费在执行上，等待时间不多，这种情况基于等待的分析作用就不大，反之亦然。如果都不确定，那么都试试。当基于执行时间的分析发现一个任务需要花费太多时间的时候，应该深入分析下，可能会发现某些”执行时间”实际上是在等待，比如等待I/O完成。理解性能剖析mysql的性能剖析将最重要的任务展示在前面，如下：值得优化的查询性能剖析不会自动给出那些查询值得优化，一些只占总响应时间比重很小的时间是不值得优化的，对一个占总响应时间不超过5%的查询进行优化，无论如何收益也不会超过5%。如果花费了1000美元去优化一个任务，但业务的收入没有任何增加，那么可以说反而导致业务被逆优化了1000美元。如果优化的成本大于利益，就应当停止优化。异常情况某些任务即使没有出现在性能剖析输出的前面也需要优化。比如某些任务执行次数很少，的但每次执行都非常慢，严重影响用户体验。因为其执行频率低，所以总得响应时间占比并不突出。未知的未知性能剖析工具会显示可能的”丢失的时间”,指的是任务的总 时间和实际测量到的时间之间的差，这可能是有些任务没有测量到，也可 能睡测量的误差和精度问题，如果发生这类问题，必须引起重视。被掩藏的细节性能剖析无法显示所有的响应时间的分布，只相信平均值是非常危险的，它会隐藏很多信息，而且无法表达全部情况。应该输出更多响应时间的信息，比如直方图、百分比、标准差、偏差指数等。二、对应用程序进行性能剖析对系统进行性能剖析建议自上而下地进行，这样可以追踪自用户发起到服务器响应的整个流程。虽然性能问题大多数情况下都和数据库有关，但应用导致的性能问题也不少。性能瓶颈可能有很多影响因素：外部资源，比如调用了外部的Web服务或者搜索引擎。应用需要处理大量的数据，比如分析一个超大的XML文件。在循环中执行昂贵的操作，比如滥用正则表达式。使用了低效的算法，比如使用暴力搜索算法来查找列表中的项三、剖析MySQL查询剖析服务器负载服务器端的剖析很有价值，因为在服务器端可以有效地审计效率低下的查询。定位和优化”坏”查询能够线束的提升应用的性能，也能解决某些特定的难题。还可以降低服务器的整体压力。这样所有的查询都将因为减少了对共享资源的争用而受益，可以退出或者避免升级更昂贵的硬件的需求，还可以发现和定位糟糕的用户体验，比如某些极端情况。捕获MySQL的查询到日志文件中在当前版本中，慢查询日志是开销最低，精度最高的测量查询时间的空间，不需要担心开启慢查询会带来额外的I/O开销。如果长期开启慢查询日志，注意要部署日志轮转工具。或者不要长期启动慢查询日志，只在需要收集负载样本的期间开启即可。有时因为某些原因如权限不足等，无法在服务器上记录查询，有两种替代技术：通过 –processlist选项不断查看SHOW FULL PROCESSLIST的输出，济洛路查询一次出现的时间和消失时间。某些情况精度足够发现问题，但是无法捕获所有的查询。通过抓取TCP网络包，然后根据MySQL的客户端/服务端通信协议进行解析。可以先通过tcpdump将网络包数据保存到磁盘，然后使用pt-query-digest的–type=tcpdump选项来解析并分析查询。精度高，可以捕获所有查询，还可以解析更高级的协议特性，比如可以解析二进制协议，从而创建并执行服务端预解析的语句及压缩协议。分析查询日志强烈建议从现在开始就利用慢查询日志捕获服务器上的所有查询，并且进行分析。可以再一些典型的时间窗口如业务高峰期的一个小时内记录查询。如果业务趋势比较均衡，那么一分钟甚至更短的时间内捕获需要优化的低效查询也是可行的。不要直接打开慢查询日志进行分析，应该先生成一个剖析报告，建议使用哪个pt-query-digest，是分析MySQL查询日志最有力的工具，可以将查询报告保存到数据库中，以及追踪工作负载随时间的变化。剖析单条查询使用SHOW PROFILE默认禁用，可以通过服务器变量在会话级(连接)别动态地修改。1mysql&gt; SET profiling = 1;然后，在服务器上执行的所有语句，都会测量其耗费时间和其他一些查询执行状态变更相关的数据库。当一条查询提交给服务器时，，此工具会记录剖析信息到一张临时表，并且给查询赋予一个从1开始的证书标识符。1234567891011121314151617//查询列表mysql&gt; SHOW PROFILES;+---------+----------+-----------------------+|Query_ID | Duration | Query |+---------+----------+-----------------------+| 1 |0.16767900| SELECT * FROM mytable |+---------+----------+-----------------------+//查询queryid为1mysql&gt; SHOW PROFILE FOR QUERY 1;+--------------+----------+|Status | Duration |+--------------+----------+|strating | 0.000082 |+--------------+----------+|Opening talbe | 0.000459 |+--------------+----------+|............. | ........ |可以帮我们精准的定位到哪些活动花费的时间最多，从而针对性的优化。使用 SHOW STATUS该命令返回了一些计数器。既有服务器级别的全局计数器，也有基于某个连接的会话级别的计数器，在会话开始的时为0，每提交一条查询增加1。是一个有用的工具，但并不是一款剖析工具，大部分结果都只是一个计数器。使用慢查询日志慢查询日志中包含了SHOW PROFILE和SHOW STATUS所有的输出，并且还有更多的信息。所有通过pt-query-digest发现”坏”查询后，会在慢查询日志中可以获得足够有用的信息。查看报告时，其标题部分一般会有如下输出1# Query 1 ： 0 QPS,0x concurrency，ID 0xEE758C5EOD7EADEE at byte 3214____可以通过这里的字节偏移值(3214)直接跳转到日志的对应部分1tail -c +3214 /path/to/query.log | head -n100使用Performance Schema新特性，mysql5.5中还不支持查询界别的剖析信息。使用性能剖析当获得服务器或者查询的剖析报告后，怎么使用？好的剖析报告能够把潜在的问题直接显示粗来，但最终的解决方案还需要用户来决定。优化查询时，用户需要对服务器如何执行查询有较深的了解。剖析报告尽可能多的收集需要的信息、给出诊断问题的正确方向，以及为其他诸如EXPALIN等工具提供基础信息四、诊断间歇性问题间歇性的问题比如系统偶尔停顿或者慢查询，很难诊断，有些幻影问题只有在没注意到的时候发送，而且很难复现。尽量不要使用试错的方式来解决问题，这种方式有很大的风险，因为结果可能会变得更坏，这也是一种令人沮丧且低效的方式。单条查询问题还是服务器问题首先确定这是单条查询问题还是服务器问题，如果服务器上所有的程序都突然变慢，又突然变好，每一条查询页都变慢了，那么慢查询可能就不一定是原因，而是由于其他问题导致的结果。反过来说，如果服务器整体运行没有问题，只有某条查询偶尔变慢，则需要将注意力放到这条特定的查询上面。大多数情况都可以通过三种技术来解决。使用 SHOW GLOBAL STATUS以较高的频率比如一秒执行一次SHOW GLOBAL STATUS命令捕获数据，问题出现时，则可以通过某些计数器(比如Threads_running、Threads_connected、Questions和Queries)的”尖刺”或则”凹陷“来发现，这个方法比较简单，所有人都可以使用，对服务器影响小。这个命令每秒输出一行数据，可以运行几个小时或者几天，然后将结果绘制成图形，可以有效方便的发现啥是否有趋势的突变。也可以尽可能长时间的运行此命令，直到发现问题再回头来看输出结果。大多数情况下，通过输出结果都可以更明确的定位问题。使用 SHOW PROCESSLIST通过不停地捕获SHOW PROCESSLIST的输出，来观察是否有大量线程处理不正常的状态或者右其他不正常的特征。使用SHOW PROCESSLIST命令时，在尾部加上\G可以垂直的方式输出结果。使用查询日志如果需要通过查询日志发现问题，需要开启慢查询日志并且在全局级别设置time为0，并且要确认所有的连接都采用了新的设置，这可能需要重置所有连接以使新的全局设置生效。要注意找到吞吐量突然下降时间段的日志。查询是在完成阶段才写入到慢查询日志的，所所以会堆积造成大量查询处于完成阶段。直到阻塞其他查询的资源占用者释放资源后，其他的查询才能执行完成。这种特征的好处是，当遇到吞吐量突然下降时，可以归咎于吞吐量下降后完成的第一个查询理解发现的问题可视化的数据最具有说服力，在实际情况中，利用上面的工具诊断时，可能会产生大量的输出结果。可以选用gnuplot或者R，或者其他绘图工具将结果绘制成图形。建议诊断问题时先使用前两种方法：SHOW STATUS和SHOW PROCESSLIST。这两种方法开销很低，可以通过简单的shell脚本或者反复执行的查询来交互式地收集数据，分析慢查询日志则相对要困难一点，经常会发现蛛丝马迹，但仔细研究时可能又消失，1这样我们很容易会认为其实没有问题。捕获诊断数据当出现间歇性问题时，需要尽可能地多收集所有的数据，而不只是问题出现时的数据，虽然这样会收集大量的诊断数据，但总比真正能够诊断问题的数据没有被收集到的情况要好。开始之前，要搞清楚两件事情：一个可靠且实时的”触发器”，也就是能够区分出什么时候问题出现的方法。一个手机诊断数据的工具。诊断触发器非常重要，这是在问题出现时能够捕获数据的基础，有两个常见的问题可能会导致无法达到预期的结果：误报或者漏检。误报是指收集了很多诊断数据，但期间其实没有发生问题，这可能浪费时间，而令人沮丧。漏检则是指在问题出现时没有捕获到数据，错失了机会，一样浪费时间。好的触发器标准是什么？Threads_running的趋势在出现问题时会比较敏感，而没有问题时则比较平稳。SHOW PROCESSLIST中线程的异常状态尖峰也是个不错的指标。选择一个合适的阈值很重要，既要足够高，以确保在正常时不会触发；又不能太高，要确保问题发生时不会错过。需要收集什么样的数据尽可能收集所有能收集的数据，但只在需要的时间段内手机。包括系统的状态、CPU利用率、磁盘使用率和可用空间、ps的输出采样、内存利用率、已经可以从MySQL获得的信息，例如SHOW STATUS、SHOW PROCESSLIST和SHOW INNODB STATUS。执行时间包括用于工作的时间和等待的时间。当一个未知问题发生时，一般来说有两种可能：服务器需要做大量的工作，从而导致大量消耗CPU；或者在等待某些资源被释放。所以需要用不同的方法收集诊断数据，来确认是何种原因。解释结果数据第一，检查问题是否真的发生，因为有很多样本数据需要检查，如果是误报就会白白浪费大量的时间。第二，是否有非常明显的跳跃性变化。查看异常的查询或事务的行为，以及异常的服务器内部行为通常都是最有收获的。查询或事务的行为可以显示是否是由于使用服务器的方式导致的问题：性能低下的SQL查询、使用不当的索引、设计糟糕的数据库逻辑架构等。通过抓取TCP流量或者SHOW PROCESSLIST输出，可以获得查询和事务出现的地方，从而知道用户对数据库进行了什么操作。如果遇到无法解释的错误，则最好将收集到的所有数据打包，提交给技术支持人员进行分析。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[16.高性能MySQL(二)]]></title>
    <url>%2F16.%E9%AB%98%E6%80%A7%E8%83%BDMySQL(%E4%BA%8C).html</url>
    <content type="text"><![CDATA[一、什么是基准测试？是mysql新手和专家都需要掌握的技能。简单来说，基准测试是针对系统设计的一种压力测试。通常是为了掌握系统的行为。但也有其他原因，如重现某个系统状态，或者是做新硬件的可靠性测试。二、为什么需要基准测试？是唯一方便有效的、可以学习系统在给定的工作负载下会发生什么的方法。可以观察系统在不同压力下的行为，评估系统的荣来，掌握哪些是重要变化，或者观察系统如何处理不同的数据。可以在系统实际负载之外创造一些虚构场景进行测试，可以完成以下工作，或者更多：验证基于系统的假设，确认这些假设是否符合实际情况。重现系统中的某些异常行为，以解决这些异常。测试系统当前的运行状态。模拟比当前系统更高的负载，以找出系统随着压力增加而可能遇到的扩展性瓶颈。规划未来业务增长。测试应用适应可变环境的能力。测试不同的硬件、软件和操作系统配置。证明新采购的设备是否配置正确。主要问题在于其不是真实压力的测试，真实压力复杂多变，基准测试较为简单，所以使用真实压力测试，可能难以从结果中分析出确切的结论。我们只能进行大概的测试，来确定系统大致的余量有多少。要尽量简单直接，结果之间容易比较，成本底且易于执行。三、基准测试的策略主要有两种主要的策略，：一个针对整个系统的整体测试，另外是单独测试mysql。这两个策略也被称为集成式以及单组件式基准测试。集成测试：测试整个应用系统，包括web服务器、应用代码、网络和数据库是非常有用的，因为用户关注的并不仅仅是mysql本身的性能，而是应用整体的性能。mysql并非应用的瓶颈。通过整体测试可以揭示这一点。整体应用的集成式测试更能揭示应用的真实表现。而单独组件的测试很难做到这一点。单组件测试需要比较不同的schema或查询的性能。针对应用中某个棘突问题的测试。为了避免漫长的基准测试，可以通过一个短期的基准测试，做快速的”周期循环”，来检测出某些调整后的效果、四、测试指标吞吐量：单位时间内事务的处理树。经典的数据库应用测试指标。主要针对在线事务处理(OLTP的吞吐量,非常适用于多用户的交互式应用。单位是每秒事务(TPS)，有些也采用每分钟事务(TPM)。响应时间或者延迟用于测试任务所需的整体时间。测试单位可能是微秒、毫秒、秒或者分钟，可以计算出平均响应时间、最小响应时间、最大响应时间和所占百分比，通过使用百分比响应时间来代替最大响应时间。使用图表有助于理解测试结果。可以将测试结果绘制成折线图或者散点图。并发性:一个非常重要又经常被误解和误用的指标。Web服务器的并发性不等同于数据库的并发性，而仅仅指表示会话存储机制可以处理多少数据的能力。Web服务器的并发性更准确的度量指标，应该是在任意时间有多少同时发生的并发请求。需要关注的是正在工作中的并发操作，或者是同事工作中的线程数或者连接数。当并发性增加时，需要测量吞吐量是否下降，响应时间是否增长，如果是这样，应用可能就无法处理峰值压力。可扩展性给系统增加一倍的工作，在理想情况下就能获得两倍的结果，或者说给系统增加一倍的资源(比如两倍的CPU树)，就可以获得两倍的吞吐量。大多数系统无法做到如此理想的线性拓展，随着压力变化。吞吐量和性能都可能越来越差。五、基准测试方法这部分不做过多解释，如果是DBA或者测试可以阅读书籍详细观看。主要有五个流程：设计和规划基准测试首先获得需要生产数据集的快照。针对数据运行查询，建立一个单元测试集作为初步的测试，并运行多边，更好的方法是选择一个有代表性的时间段，比如高峰期一小时，或者一整天，记录生产系统所有查询。如果时间段比较小。可以选择多个时间段，这样有助于覆盖整个系统的活动状态，例如每周报表查询、或者非峰值时间运行的批处理作业。可以在不同级别记录查询。例如，如果是集成式基准测试，可以记录Web服务器上的HTTP请求，也可以打开mysql的查询日志。基准测试应该运行多长时间应该运行足够长的时间，需要等待系统处于稳定状态，而达到稳定状态需要非常多的时间。获取系统性能和状态获得准确的测试结果集运行基准测试并分析结果六、基准测试工具ab是一个Apache HTTP服务器基准测试工具。它可以测试HTTP服务器每秒最多可以处理多少请求。如何测试的是Web应用服务，这个结果可以转成整个应用每秒可以满足多少请求，非常简单但用途有限，只能针对单URL进行测试。http_load也被设计为Web服务器进行测试，但比ab更加灵活，可以通过一个输入文件提供多高URL，http_load在这些URL中随机选择进行测试。也可以定制http_load,使其按照时间的比率进行从厕所，而不仅仅是测试最大请求处理能力。JMeter是一个Java应用程序，可以加载其他应用并测试其性能。它虽然是设计用来测试Web应用的，但也可以用户测试其他诸如FTP服务器器，或者通过JDBC进行数据库查询测试。比ab和http_load的复杂的多，可以通过控制预热时间等参数，更加灵活地模拟真实用户的访问。拥有绘图接口，还可以对测试进行金鸡路，然后离线重演测试结果。mysqlslap可以模拟服务器的负载，并输出计时信息，测试时可以执行并发连接数，并指定SQL语句。如果没有指定SQL语句。会自动生成查询schema的SELECT语句。MySQL Benchmark Suite(sql-bench)单线程，用于测试服务器执行查询的速度，可以用于不同数据库服务器上进行比较测试。单用户模式，测试数据集很小且用户无法使用指定的数据。Super Smack用于mysql和PostgerSQL的基准测试工具。可以提供压力测试和负载生成，可以模拟多用户访问，可以加载测试数据到数据库，并支持使用随机填充测试表。sysbench多线程系统压测工具，可以根据影响数据库服务器性能的各种因素来评估系统的性能。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[15.高性能MySQL(一)]]></title>
    <url>%2F15.%E9%AB%98%E6%80%A7%E8%83%BDMySQL(%E4%B8%80).html</url>
    <content type="text"><![CDATA[一、逻辑架构1. 连接管理与安全性第一层:大多基于网络客户端/服务器的工具或者服务都有类似的架构，比如连接处理，授权认证，安全等等。第二层：mysql核心功能都在这一层，包括查询解析、分析、优化、缓存以及所有的内置函数。所有的跨存储引擎功能都在这一层实现，比如触发器、存储过程、视图等。第三层：包含存储引擎，负责mysql中的数据的存储和提取。不会解析sql，不同存储引擎也不会相互通信，只是简单的响应上层服务器的请求。2.优化与执行每个客户端连接都会在服务器进程中都有一个线程，这个连接的查询只会在这个单独的线程中执行，服务器会缓存线程，不需要新建和销毁。第一步:当客户端连接到服务器时，服务器对其进行认证，基于用户名，密码，原始主机信息，连接成功后，会继续验证该客户端是否有执行某个特定查询的权限。第二步：认证通过后，mysql会解析查询，并创建内部数据结构，对其进行优化，包括重写查询，决定表的读取顺序，以及选择合适的索引。用户可以通过关键字(hint)提示优化器，也可以通过explain请求服务器解释优化过程。第三步：对于select查询，在解析查询之前，服务器会先检查查询缓存，如果能够在其中找到对应的查询，服务器就不必再执行查询解析、优化和执行的整个过程，而是直接返回查询缓存中的结果集。二、并发控制1.读写锁在处理并发读或者写的时候，可以通过实现一个由两种类型的锁组成的锁系统来解决问题。通常被称为共享锁和排他锁，也叫读锁和写锁。读锁(共享锁)：读锁是共享的，或者说是相互不阻塞的。多个客户在同一时刻可以同时读取同一个资源。而互不干扰。写锁(排它锁)：一个写锁会阻塞其他的写锁和读锁，这个处于安全策略的考虑，只有这样，才能确保在给定的时间里，只有一个用户能执行写入，并防止其他用户读取正在写入的统一资源。在实际数据库系统中，锁时时刻刻都在发生，当某个用户正在修改一部分数据时，mysql会通过锁定防止其他用户读取统一数据。2.锁粒度提高共享资源并发性的方式就是让锁定对象更有选择性。尽量只锁定需要修改的部分数据，而不是所有的资源。而加锁也需要消耗资源。锁的各种操作，包括获得锁、检查锁是否已经被解除、释放锁都会增加系统开销。所谓所策略，就是在锁的开销和数据的安全性之间寻找平衡，一般都是在表上施加行级锁。表锁：最基本的锁策略，并且是开销最小的策略，它会锁住整张表。一个用户在对表进行写操作(插入、删除、更新等)前，需要先获得写锁，这会阻塞其他用户对该表的所有读写操作。只有没有写锁时，其他读取用户才能获取读锁，读锁之间是不相互阻塞的。特定场景中，表锁也可能有良好的性能。例如，READ LOCAL表锁支持某些类型的并发操作。另外，写锁也比读锁有更高的优先级，因此一个写锁请求可能会被插入到读锁队列前面，反之不不能。行级锁：可以最大程度地支持并发处理(同时也带来了最大的锁开销)。众所周知，在InnoDB和XtraDB，以及其他的一些存储引擎中实现了行级锁。行级锁只在存储引擎层实现，而mysql服务器层没有实现。三、事务事务就是一组原子性的sql操作，或者说一个独立的工作单元。如果数据库引擎能够成功的对数据库应用该组操作的全部语句，那么就执行该组操作，如果其他有任何一条语句因为崩溃或者其他原因无法执行，那么所有语句都不会执行。一句话总结：事务内的语句，要么全部执行成功，要么全部执行失败.1.ACID:原子性(atomicity):一个事务必须被视为一个不可分割的最小工作单元，不可能只执行其中的一部分，这就是事务的原子性。一致性(consistency)：数据库总是从一个一致性的状态转换到另外一个一致性的状态，事务中的操作要么全部执行成功，要么全部失败回滚。隔离性(isolation)一个事务所做的修改在最终提交之前，对其他事务是不可见的。持久性(durability)一旦事务提交，对其所做的修改就会永久保存到数据库中。此时即使系统崩溃，修改的数据也不会丢失。2.隔离级别：READ UNCOMMITTED(未提交读)：在未提交读级别，事务中的修改，即使没有提交，对其他事务也都是可见的。事务可以读取未提交的数据，这也称为脏读。这个级别会导致很多问题，从性能上来说，不会比其他级别好太多，缺缺乏其他级别的很多好处，一般很少使用。READ COMMITTED(提交读/不可重复读)解决了脏读的问题。大多数数据默认的隔离级别，mysql不是。简单定义：一个事务开始时，只能“看见”已经提交的事务所做的修改。换句话说，一个事务从开始直到提交之前，所做的任何修改对其他事务都不可见的。两次执行同样的查询，可能会得到不一样的结果，产生了不可重复读的问题。REPEATABLE READ(可重复读)：解决了脏读和不可重复读的问题。mysql的默认隔离级别，该级别保证了再同一事务中多次读取同样记录的结果是一致的。但是无法解决幻读的问题。所谓幻读，指的是当某个事务在读取某个范围内的记录时，另一个事务又会在该范围内插入新的记录，当之前的事务再次读取该范围内的技术时，会产生幻行。InnoDB和XtraDB存储引擎通过多版本并发控制，解决幻读问题。SERIALIZABLE(可串行化)：最高的隔离级别，通过强制事务串行执行，避免了前面说的幻读的问题，会在读取的每一行都加上所，所以可能导致大量的超时和锁争用的问题。实际中很少用到，只有在非常需要确保数据的一致性而且没有并发的情况下，才考虑采用该级别。总结：3.死锁两个或者多个事务在同一资源上相互占用，并请求锁定对方占用的资源，从而导致恶性循环的现象。当多高事务试图以不同的顺序锁定资源时，就可能产生死锁。多个1事务同事锁定同一个资源时，也会产生死锁。如何解决死锁？数据库系统实现了各种死锁检测和死锁超时机制。越复杂的系统，比如InnoDB存储引擎，越能检测出死锁的循环依赖，并立即返回一个错误。这种解决方法很有效。另一种解决方式，就是当查询的时候达到锁等待超时的设定后放弃锁请求，这种方法不太好。InnoDB目前处理死锁的方法是，将持有最少行级排它锁的事务进行回滚。死锁发生后，只有部分或者完全回滚其中的一个事务，才能打破死锁。大多数情况下只需要重新执行因死锁回滚的1事务即可。4.事务日志提高事务效率，使用时，存储引擎在修改表的数据时只需要修改其内存拷贝，再把该修改行为记录到持久在硬盘上的事务日志中，而不需要每次都将修改的数据本社持久到磁盘。事务日志采用追加的方式，因此写日志的操作是在磁盘上一小块区域内的顺序I/O，事务日志持久以后，内存中被修改的数据在后台可以慢慢地刷回到磁盘，通常称为预写式日志，修改数据需要写两次磁盘。如果日志持久化但是数据没有写回磁盘时，系统奔溃，存储引擎在重启后能够自动恢复这部分被修改的数据。5.mysql中的事务提供两种事务型的存储引擎：InnoDB和NDB Cluster。默认采用自动提交模式，如果不是显式地开始一个事务，则每个查询都被当做一个事务执行提交操作。InnoDB采用的是两阶段锁定协议。在事务执行过程中，随时都可以执行锁定，锁只有在执行COMMIT或者ROLLBACK的时候才会被释放，并且所有的锁是在同一时刻被释放。四、多版本并发控制(MVCC)以InnoDB为主，是通过在每行记录后面保存两个隐藏的列来实现的。这两个列，一个保存了行的创建时间，一个保存行的过期时间(或删除时间)。当然存储的不是实际的时间值，而是系统版本号。每开始一个新的事务，系统版本号会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询道德每行记录的版本号进行比较。下面看一下，在可重复读的级别下，MVCC具体操作。1.SELECTInnoDB会根据以下两个条件检查每行记录：a.InnoDB只查找版本早于当前事务版本的数据行(也就是行的系统版本号小于或者等于事务的系统版本号)，这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过得。b.行的删除版本要么未定义，要么大于当前事务版本。这样可以确保事务读到的行，在失误开始之前未被删除。只有符合符合上述两个条件的记录，才能返回作为查询结果。2.INSERTInnoDB为新插入的每一行保存当前系统版本号作为行版本号。3.DELETEInnoDB为删除的的每一行保存当前系统版本号作为行删除标识。4.UPDATEInnoDB为插入一行新记录，保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为行删除标识。、优缺点：保存这两个额外的系统版本号，使大多数读操作都可以不用加锁，操作简单，性能很好，能够保证只会读到符合标准的行。不足在于每个记录都需要额外的存储空间，需要做更多的行检查工作，以及一些额外的维护工作。mysql只在可重复读和提交读两个隔离机制下工作。五、存储引擎1.InnoDBmysql默认的事务型引擎，数据存储在表空间，它是InnoDB管理的黑盒子，由一系列的数据文件组成，可以将每个表的数据和索引存放在单独文件中。采用MVCC来支持高并发，并且实现1李四光标准的隔离级别，默认是可重复读，并且可以通过间隙锁策略防止幻读的出现。间隙锁使得InooDB不仅仅锁定查询涉及的行，还会对索引中的间隙进行锁定，防止幻影行的插入。表基于聚簇索引建立，对主键查询有很高的性能，不过它的耳机索引中必须包含主键列。所以主键列很大的话，其他索引都会很大。如果表索引较多，主键应当尽可能的小。内如做了很多优化，包括从磁盘读取数据时采用的可预测性玉都，能够自动在内存中创建hash索引以加速读操作的自适应哈希索引，以及能够加速插入操作的插入缓冲区。2.MyISAMmysql5.1版本以前的默认索引，提供大量特性，包括全文索引、压缩、空间函数，但是不支持事务和行级锁，崩溃后无法安全修复对于只读的数据，或者表比较小、可以忍受修复操作，可以使用MyISAM。特性：加锁与并发针对整张表加锁，而不是针对行，读取时会读到所有表加共享锁，写入时则对表加排他。但是在标有读取查询的同时，也可以往表里插入新的数据。修复可以收工或者自动执行检查和修复操作，执行表的修复可能导致一些数据丢失，而且修复操作非常慢，可以通过CHECK TABLE mytalbe检查表的错误，如果有错误，可以通过执行REPAIR TABLE mytalbe 进行修复。索引特性既是是BLOB和TEXT等长字段，也可以基于前500个字符创建索引，MyISAM也支持全文索引，这是一种基于分词创建的索引，可以支持复杂的查询。延迟更新索引创建表时如果指定了DELAY_KEY_WRITE选项，在每次修改执行完成时，不会立刻将修改的索引数据写入磁盘，而是会写到内存中的键缓冲区，只有在清理键缓冲区或者关闭表的时候才会将对应的索引2块写入磁盘，极大的提升写入性能，但是数据库或系统崩溃会造成索引损坏。3.mysql内建的其他存储引擎Archive引擎只支持INSERT和SELECT操作。Blackhole引擎，没有实现任何的存储机制，它会丢弃索引插入的数据，不做任何保存。CSV引擎可以将普通的CSV文件作为mysql的表来处理，但这种表不支持索引。Federated引擎方位其他mysql服务器的一个代理，它会创建一个到远程mysql服务器的客户端连接，并将查询传输到远程服务器执行，然后提取或者发送需要的数据。Memory引擎如果需要快速地访问数据，并且这些数据不会被修改，重启后丢失也没有关系，则使用Memory表是非常有用的。至少比MyISAM表快一个数量级，因为所有数据数据都保存在内存中，不需要进行磁盘I/O。Merge引擎是MyLSAM引擎的变种，由多个MyISAM表合并而来的虚拟表，引入分区后，该引擎已经被放弃。NDB集群引擎mysql服务器，NDB集群存储引擎，以及分布式的、share-nothing的、容灾的、高可用的NDB数据库的组合，被称为msql集群。4.选择合适的存储引擎大多数情况下，InooDB都是正确的选择，所以在mysql5.5版本后将InnoDB作为默认的存储引擎了，。简单归纳一句话：”除非需要用到某些InnoDB不具备的特性，并没有其他办法代替，否则都应该优先选择InnoDB”。例如用到全文索引，建议优先考虑InnoDB加上Sphinx的组合，而不是支持全文索引的MyISAM。5.转换表的引擎ALTER TABLE将表从一个引擎修改为另一个引擎最简单的方法就是使用ALTER TABLE语句1mysql&gt; ALTER TABLE mytable ENGINE = InnoDB;需要执行很长时间，原表会加上读锁，会失去和原引擎先关的所有特性。导出与导入使用mysqldump工具将数据到处到文件，然后修改文件中CREATE TABLE语句的存储引擎选择，注意同时修改表名。同时要注意mysqldump默认会自动在CREATE TABLE语句前面加上DROP TABLEy语句，不注意这一点可能会导致数据丢失。创建与查询综合了第一种和第二张方法，不需要导出整个表数据，而是先创建一个新的存储引擎的表，然后利用INSERT—-SELECT语法来导数据：123mysql&gt;CREATE TABLE innodb_table LIKE myisam_table;mysql&gt;ALTER TABLE innodb_table=InnoDB;mysql&gt;INSERT INTO innodb_table SELECT * FROM myisam_table;数据量很大，可以考虑做分批处理，针对每一段时间只需事务提交操作，以避免大事务产生过多的uudo。假设有主键字段id，重复运行以下语句(最小值x和最大值y)将数据导入新表：123mysql&gt; START TRANSACTION;mysql&gt; INSERT INTO innodb_table SELECT * FROM myisam_table-&gt; WHERE id BETWEEN x AND y;mysql&gt; COMMIT;这样操作完成以后，新表是原表的一个全量复制，原表还在，如果需要可以删除原表。如果有必要，可以在执行的过程中对原表加锁，以确保新表和原表的数据一致。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[7.UML类图]]></title>
    <url>%2F7.UML%E7%B1%BB%E5%9B%BE.html</url>
    <content type="text"><![CDATA[一、什么是UML图？UML（Unified Modeling Language）是一种统一建模语言，为面向对象开发系统的产品进行说明、可视化、和编制文档的一种标准语言。UML图分为用例视图、设计视图、进程视图、实现视图和拓扑视图，又可以静动分为静态视图和动态视图。静态图分为：用例图，类图，对象图，包图，构件图，部署图。动态图分为：状态图，活动图，协作图，序列图。二、UML类图用户根据用例图抽象成类，描述类的内部结构和类与类之间的关系，是一种静态结构图。 在UML类图中，常见的有以下几种关系: 泛化（Generalization）, 实现（Realization），关联（Association)，聚合（Aggregation），组合(Composition)，依赖(Dependency)。各种关系的强弱顺序： 泛化 = 实现 &gt; 组合 &gt; 聚合 &gt; 关联 &gt; 依赖泛化：是一种继承关系，指定了子类如何继承父类的所有特征和行为。实现+空心三角箭头表示。实现：是一种类与接口的关系，表示类是接口所有特征和行为的实现。虚线+空心三角箭头表示。关联：是一种拥有关系，它使一个类知道另一个类的属性和方法。可以是单向也可以是双向。双向关联可以有两个箭头或者没有箭头，单向的关联有一个箭头。聚合：是整体与部分的关系，且部分可以离开整体而单独存在。实线+空心菱形表示组合：是整体与部分的存在，但是部分不能离开整体而单独存在。实现+实心菱形表示依赖：是一种使用的关系，一个类的实现需要另一个类的协助，尽量避免双向协助。带箭头的虚线，指向被使用者三、示例图]]></content>
      <categories>
        <category>随记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[6.一句话设计模式]]></title>
    <url>%2F6.%E4%B8%80%E5%8F%A5%E8%AF%9D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F.html</url>
    <content type="text"><![CDATA[创造类单例模式：确保一个类只有一个实例，而且自行实例化并像整个系统提供这个实例。私有化构造方法，提供公共方法获得实例。工厂方法模式：定义一个用于创建对象的接口，让子类决定实例化哪个类，工厂方法使一个类的实例化延迟到了子类。抽象工厂模式：为创建一组相关的或者相互依赖的的对象提供一个接口，而无需指定他们的具体类。建造者模式：将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示原型模式：用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。实现Cloneable接口，重写clone方法。迭代器模式：提供一个方法访问一个容器对象中的各个元素，而又不需暴露该对象的内部细节。Java中已经默认实现，使用foreach即可。行为类命令模式：把一个请求或者操作封装在命令对象中，命令模式允许系统使用不同的请求把客户端参数化，对请求排队或者记录请求日志，可以提供命令的撤销和回复功能。解释器模式：给定一种语言，定义它的文法的一种表示，并定义一个解释器，该解释器使用该表示来解释语言中的句子。责任链模式：使多个对象都有机会处理请求，从而避免了请求的发送者和接受者1之间的解耦关系。将这些对象连成一条链，并沿着这条链传递该请求，直到有对象处理它为止。观察者模式：定义对象间一种一对多的依赖关系，使得每当一个对象改变状态，则所有依赖于它的对象都会得到通知被自动更新。中介者模式：用一个中介对象封装一系列对象的交互，中介者使各对象不需要显示地相互作用，从而使其耦合松散，而且可以独立的改变它们之间的交互。备忘录模式：在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，这样以后就可以将该对象恢复到原先保存的状态。状态模式：当一个对象内在状态改变时允许其改变行为，这个对象看起来像是改变了其类。策略模式：定义一组算法，将每个算法都封装起来，并且可以使他们之间可以相互转换。模板方法模式：定义一个操作中的算法骨架，而将一些步骤延伸到子类，使得子类可以不改变一个算法结构即可重定义该算法的某些特定步骤。访问者模式：封装一些作用于某种数据结构中各元素的操作，它可以在不改变数据结构的前提下定于作用于这些元素的新操作。结构类适配器模式：将一个类的接口变换成客户端所期待的另一个接口，从而使本因接口不必配而无法工作的两个类能够在一起工作。组合模式：将对象组合成树形结构以表示”部分-整体”的层次结构，使得用户对单个对象和组合对象的使用具有一致性。代理模式；为其他对象提供一种代理以控制这个对象的访问。桥梁模式：将抽象和实现解耦，使得两者可以独立的变化。装饰模式：动态的给一个对象添加一些额外的职责，就增加功能来说，它相比生成子类更为灵活门面模式：要求一个子系统的外部与内部得同学都必须通过一个统一的对象进行，门面模式提供一个搞层次的接口，使得子系统更易于使用。享元模式：使用共享对象可以有效的支持大量的细粒度的对象。代码相关链接：《大话设计模式》 《设计模式之禅》]]></content>
      <categories>
        <category>随记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[9.Resful相关]]></title>
    <url>%2F9.Resful%E7%9B%B8%E5%85%B3.html</url>
    <content type="text"><![CDATA[一、什么是Resful？RESTful是一种软件架构风格、设计风格，而不是标准，只是提供了一组设计原则和约束条件。它主要用于客户端和服务器交互类的软件。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制。二、常用方法：1234GET /blog/getArticles --&gt; GET /blog/Articles 获取所有文章GET /blog/addArticles --&gt; POST /blog/Articles 添加一篇文章GET /blog/editArticles --&gt; PUT /blog/Articles 修改一篇文章 GET /rest/api/deleteArticles?id=1 --&gt; DELETE /blog/Articles/1 删除一篇文章三、一句话总结！URL定位资源,HTTP动词描述操作。四、如何在resful传入多个参数？1@PutMapping(value = &quot;wx/createCustomer/&#123;number&#125;/&#123;nickName&#125;.do&quot;)五、@PathVariable 和 @RequestParam区别：@PathVariable用来绑定url模板变量值，获取url请求中的动态参数12345678910@RequestMapping(&quot;/zyh/&#123;type&#125;&quot;) public String zyh(@PathVariable(value = &quot;type&quot;) int type) throws UnsupportedEncodingException &#123; String url = &quot;http://wx.diyfintech.com/zyhMain/&quot; + type; if (type != 1 &amp;&amp; type != 2) &#123; throw new IllegalArgumentException(&quot;参数错误&quot;); &#125; String encodeUrl = URLEncoder.encode(url, &quot;utf-8&quot;); String redirectUrl = MessageFormat.format(OAUTH_URL, WxConfig.zyhAppId, encodeUrl, &quot;snsapi_userinfo&quot;, UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;)); return &quot;redirect:&quot; + redirectUrl; &#125;@RequestParam控制层用来获取参数12345678910111213141516171819@Controller@RequestMapping(&quot;/wx&quot;)public class WxController &#123; @Autowired private WxService wxService; private static final Log log= LogFactory.getLog(WxController.class); @RequestMapping(value = &quot;/service&quot;,method = RequestMethod.GET) public void acceptWxValid(@RequestParam String signature, @RequestParam String timestamp, @RequestParam String nonce, @RequestParam String echostr, HttpServletResponse response) throws IOException &#123; PrintWriter out = response.getWriter(); if (SignUtil.checkSignature(signature, timestamp, nonce)) &#123; out.print(echostr); &#125;else out.print(&quot;fail&quot;); out.flush(); out.close(); &#125;]]></content>
      <categories>
        <category>随记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[11.数据结构与集合(下)]]></title>
    <url>%2F11.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E9%9B%86%E5%90%88(%E4%B8%8B).html</url>
    <content type="text"><![CDATA[一、Map集合Map类特点:Map类取代了旧的抽象对象Dictionary，拥有更好的性能。没有重复的key，可以有重复的value。Value可以是List、Set、Map类对象。AbstractCollection只实现了remove、clear操作，没有修改和删除KV是否允许有空值，以实现类约束为准。HashMap允许KV都为空，ConcurrentHashMap不允许KV为空二、 树常用的数据机构，它是一个由有限节点组成的一个具有层次关系的集合，数据就存在树的节点中。最顶层只有一个节点成为根节点。结构特点：一个节点，既是只有根节点，也是一棵树。其中任何一个节点与下面所有节点构成的树称为字数。根节点没有父节点，而叶子节点没有子节点。除根节点外，任何节点有且只有一个父节点。任何节点可以有0~n个子节点。最多有两个节点的树称为二叉树，其中最重要的概念是平衡二叉树、二叉查找树、红黑树。1. 平衡二叉树性质：树的左右高度差不能超过1。任何往下递归的左子树与右子树，必须符合第一条性质。没有任何节点的空树或者只有根节点的树也是平衡二叉树。2. 二叉查找树擅长数据查找，对树增加了额外的要求：对于任意节点来说，它的左子树所有节点的值都小于它，而它的右字数所有节点都必须大于它。常见的遍历方式有三种：前序遍历、中序遍历、后续遍历。它们三者规律如下：在任何递归子树中，左节点一定在右节点之前先遍历。前序遍历的顺序是根节点、左节点、右节点；中序遍历的顺序是左节点、根节点、右节点；后序遍历的顺序是左节点、右节点、根节点。3. AVL树一种平衡二叉查找树，增加和删除节点通过树形旋转重新达到平衡。左旋是以某个节点为中心，将它沉入当前左节点的位置，而让当前的右节点成为新树的根节点，也称为逆时针旋转。右旋是以某个节点为中心，将它沉入当前右子节点的位置，而让当前的左节点成为新树的根节点，也称为顺时针旋转。4. 红黑树主要特征是在每个节点上增加一个属性来表示节点的颜色以是红色，也可以是黑色。和AVL树类似，都是在进行插入和删除元素时，通过特定的旋转来保持自首平衡的，从而获得了较高的查找性能。和AVL树相比，红黑树并不追求所有递归子树的高度差不超过1.而是保证从根节点到叶子节点的最长路径不超过最短路径的2倍，通过重新着色和左右旋转，更加高效的完成了插入和删除操作后的自平衡调整。5个约束条件：节点只能是红色或黑色。根节点必须是黑色。所有NIL节点都是黑色的。NIL，即叶子节点下挂的两个虚节点。一条路径上不能出现相邻的两个红色节点。在任何递归子树内，根节点到叶子节点的所有路径上包含相同数目的黑色节点。总结一下：”有红必有黑，红红不相连”。如果一个树的左子节点或者右子节点不存在，则均认定为黑色。红黑树的任何旋转在3次只能均可完成。5. 红黑树和AVL比较:面对频繁插入和删除，红黑树更为合适；面对低频修改、大量查询，AVL树更为合适。三、TreeMap按照Key的排序结果来组织内部结构的Map类集合，它改变了Map类散乱无序的形象。虽然没有ConcurrentHashMap和HashMap普及，但是在Key有排序要求的情况下事半功倍。在继承树中，有两个与众不同的接口：SortedMap和NavigableMap。SortedMap表示它的key是有序不可重复的，支持获得头尾的key-value元素。插入的Key必须实现Comparable接口，所以Key不允许为空，Value可以为空。NavigableMap继承SortedMap接口，根据指定的搜索条件返回最匹配的Key-Value元素。不同于HashMap，TreeMap并非一定要重写hashcode和equals方法来达到Key去重的目的。插入新节点，三个前提条件：需要调整的新节点总是红色的。如果插入新节点的父节点是黑色的，无需调整。如果插入新节点的父节点是红色的，因为红黑树规定不能出现相邻的两个红色节点，所以进入循环判断，或重新着色，或左右旋转，最终达到红黑树的五个约束条件，退出条件如下：1while(x != null &amp;&amp; x!= root &amp;&amp; x.parent.color == RED)&#123;...&#125;如果是根节点，则直接退出，设置为黑色即可；如果不是根节点，并且父节点是红色，则一直调整，直到退出循环。TreeMap的插入操作就是按照Key的对比往下遍历，大于比较值节点的往右走，小于比较值节点的往左走,先按照二叉查找树的特性进行操作，无需关心节点颜色与树的平衡，后续会重新着色旋转，保持红黑树的特性。四、HashMap三个存储概念：名称说明table存储所有节点数据的数组slot哈希槽。即table[i]这个位置bucket哈希桶。table[i]上所有元素形成的表或数的集合除了局部方法或绝对线程安全情况下，优先推荐使用ConcurrentHashMap。两者性能相差无几，但后者解决了 高并发下线程安全的问题。HashMap的死链问题以及扩容数据丢失问题是慎用HashMap的两个主要原因。默认容量16，默认负载因子0.75，当到达阈值(容量 * 负载因子)时，进行扩容。每次扩容容量为原来的两倍。发生hash冲突的情况：两节点Key值相同(hash值一定相同)，导致冲突。两节点Key值不同，但是由于hash函数的局限性导致了hash值相同，导致冲突。两节点Key值不同，hash值也不同，但是hash值对 数组长度取模后相同，导致冲突。高并发中，新增对象丢失原因：并发赋值时被覆盖。已遍历区间新增元素会丢失。“新表被覆盖”。迁移丢失。在迁移过程中，有并发时，next被提前设置为null。JDK1.7和1.8中HashMap的区别：JDK1.7中使用的是头插法，1.8中使用的是尾插法。因为JDK1.7中是用单链表进行的纵向延伸，采用头插法能够提高插入的效率，但是也会出现逆序且环形链表死循环的问题。在1.8之后加入了红黑树，采用尾插法，能够避免出现逆序且链表死循环的问题。扩容后数据存储位置计算方式也不一样:1.7直接使用hash值和需要扩容的二进制数进行&amp;(这里就是为什么扩容的时候为啥一定必须是2的多少次幂的原因所在，因为如果只有2的n次幂的情况时最后一位二进制数才一定是1，这样能最大程度减少hash碰撞)(hash值 &amp; length-1)。1.8直接用了1.7的运算规律，扩容前位置 + 扩容大小值=JDK1.8的计算方式，只需要判断Hash值的新增参与运算的位是0还是1就直接迅速算出扩容后的存储方式。JDK1.7是数组 + 单链表的数据结构，1.8之后，使用的是数组 + 链表 + 红黑树的数据结构(当链表的深度到达8时，就会自动扩容把链表转换成红黑树的数据结构来把时间复杂度从O(N)变成O(LogN)提高了效率；当阈值小于6时，红黑树转换成链表)。为什么HashMap是线程不安全的？HashMap在并发时出现的问题主要是两方面：put的时候导致的多线程数据不一致两个线程A和B，A先插入key-value到HashMap中，计算出hash桶的索引坐标，获得该桶的链表头节结点，A执行完后B开始执行插入，假设A计算出来的hash桶索引和B计算出的hash桶索引一样，在B插入后，A再次被调度时，执行了旧数据，覆盖了B的插入记录，这样线程B插入的数据就消失了，从而造成数据不一致。resize而引起死循环当两个线程同时检测到元素个数超过了数组大小 x 负载因子，同时会在put()方法中调用resize(),两个线程同时修改一个链表结构会产生一个循环列表(JDK1.7中，会出现resize前后元素顺序倒置的情况)。接下来再想通过get()获取一个元素，就会出现死循环。五、ConcurrentHashMap高并发下其他哈希式集合：HashTable是JDK1.0引入的集合，以全互斥的方式处理并发情况，性能极差。HashMap是JDK1.2引入的，非线程安全，最大的问题是在并发写的情况下，容易出现死链，导致服务不可用。ConcurrentHashMap是JDK1.5引入的线程安全的哈希式集合。ConcurrentHashMap设计理念；JDK1.8之前：采用分段锁的设计理念，相当于HashTable和HashMap的折中版本，把数据分成一段一段进行存储，给每一段分配一把锁，当线程占用锁访问其中一个数据时候，其他端的数据也能被其他线程方法，实现真正的并发访问。优点：写操作的时候可以只对元素所在的Segment进行加锁即可，不会影响到其他的Segment，并发能力大大提高。缺点：Hash过程要比普通的HashMap要长。JDK1.8之后：参考了HashMap，采用了数组 + 链表 + 红黑树的实现方式来设计，内部大量的采用了CAS操作，彻底放弃了Segment转而采用的是Node。更多知识点会在后面专题介绍，尽情期待。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10.数据结构与集合(上)]]></title>
    <url>%2F10.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E9%9B%86%E5%90%88(%E4%B8%8A).html</url>
    <content type="text"><![CDATA[一、数据结构定义指逻辑意义上数据组织方式及其相应的处理方式数据结构=逻辑结构+存储结构+（在存储结构上的）运算/操作数据结构是指相互之间存在一种或者多种特定关系的数据元素的集合。是组织并存储数据以便能够有效使用的一种专门格式，它用来反映一个数据的内部构成，即同一个数据由那些成分数据构成，以什么方式构成，是什么结构。数据的逻辑结构数据的逻辑结构指数据元素之间的逻辑关系（和实现无关）分类1：线性结构和非线性结构线性结构有且只有一个开始结点和一个终端结点，并且所有的结点都最多只有一个直接前驱和一个直接后驱。线性表就是一个典型的线性结构，它有四个基本特征：集合中必存在唯一的一个“第一元素”；集合中必存在唯一的一个“最后的元素”；除最后元素外，其他数据元素均有唯一的“后继”；除第一元素外，其他数据元素均有唯一的“前驱”。非线性结构：一个结点元素可能对应多个直接前驱和多个直接后驱。常见的有二叉树，图等。分类2：集合结构、线性结构、树状结构、网状结构集合结构该结构的数据元素之间的关系是“同属于一个集合“，别无其他关系三个特征：确定性（集合中的元素必须是确定的）唯一性（集合中的元素互不相同）、无序性（集合中的元素没有前后之分）线性结构数据结构中线性结构指的是数据元素之间存在着“一对一”的线性关系的数据结构树状结构除了一个元素外，每一个数据元素有且只有一个直接前驱元素，但是可以有多个直接的后续元素，特点是数据元素之间是一对多。网状结构每个数据元素之间都可以有多个直接前驱元素，也可以有多个直接后继元素，特点是数据元素之间是多对多的关系。数据的存储结构数据的存储结构主要包括数据元素本身的存储结构以及数据元素之间关系表示，是数据的逻辑结构在计算机中的表示。分类：顺序储存、链式储存、索引储存以及散列储存顺序存储结构把逻辑上相邻的节点存储在物理位置上相邻的存储单元，结点之间的逻辑关系由存储单元的邻接关系来体现优点：是节省存储空间，因为分配给数据的存储单元全用存放结点的数据，结点之间没有占用额外的存储空间。采用这种方法时，可实现对结点的随机存取，即每一个结点对应一个序号，由该序号可以直接计算出来结点的存储地址。缺点：插入和删除需要移动元素，效率较低。链式储存结构数据元素的存储空间对应的不连续的存储空间，每个存储空间节点对应一个需要存储的元素。每个结点是由数据域和指针域组成。元素之间的逻辑关系通过存储节点之间的链接关系反映出来。特点：比顺序存储结构的存储密度要小；逻辑上相邻的节点物理上不必相邻；插入、删除灵活，不需要改变位置，只需要改变指针中的地址；查找结点是链式存储要比顺序存储慢。索引储存结构除了建立存储结点信息外，还建立附加的索引来标识结点的位置散列存储结构根据结点的关键字直接计算出该结点的存储地址一种神奇的存储结构，添加，查询速度快。二、集合常见有Set,Queue，List，Map接口，全部继承Collection接口。List集合线性数据结构的主要表现，通常存在明确的上一个和下一个元素，也UC你在明确的第一个元素和最后一个元素。常见的又ArrayList和LinkedList两个集合类。ArrayList：容量可以改变的非线程安全集合。内部使用数组存储，集合扩容时会创建更大的数组空间，把原有的数组复制到新数组中。优点：能够快速的随机访问缺点: 插入和删除时候速度很慢，需要移动元素。LinkedList:本质是双向链表，和ArrayList相比，插入和删除速度更快，但是随机访问速度慢。这个接口同时有队列和栈的性质，包含3个重要成员：size、first、size。size是双向链表中节点的个数。first和last分别指向第一个和最后 一个节点。优点：可以将零散的内存单元通过附加引用的方式关联起来，形成按链路顺序查找的线性结构，内存利用率较高。Queue集合一种先入先出的数据结构。队列是一种特殊的线性表，它只允许在表的一端进行获取操作，另一端进行插入操作。当队列为空时，称为空队列。由于其本身FIFO的特性和阻塞操作的特点，常常被作为Buffer(数据缓冲区)使用。Map集合以Key-Value键值作为存储元素实现的哈希结构，Key唯一，value可以重复。最早用于存储键值对的Hashtable因为性能瓶颈已经被淘汰，HashMap线程不安全，ConcurrentHashMap是线程安全的，在多线程中，优先使用ConcurrentHashMap。TreeMap是Key有序的Map类集合。Set集合不允许出现重复元素的集合类型。最常用的有HashSet、TreeSet和LinkedHashSet。HashSet是使用HashMap来实现的，只是value固定为一个静态对象，使用Key保证集合元素的唯一性，但是它不能保证集合元素的顺序。TreeSet是使用TreeMap来实现的，底层作为树结构，在添加新元素到集合中时，按照某种比较规则将其插入合适的位置，保证插入后的集合仍然有序。LinkedHashSet继承自HashSet，具有HashSet的优点，内部使用链表维护元素插入的顺序。三、集合初始化ArrayList如果原始容量13，当添加一个元素时，根据程序中的计算方法，得出113的二进制数右移以为后得到的二进制数110，即十进制数6，最终扩容的大小计算结果为：oledCapacitiy + (oldCapacitiy&gt;&gt;1)=13 + 6 = 19。当使用无参数的构造方法时，默认大小为10，也就说第一次add的时候，分配10的容量，每次都调用Array.copyOf方法。当需要把1000个元素放入集合时候，如果我们没有定义容量，将会产生被动扩容和数组复制的额外开销，甚至有可能导致OOM的风险。HashMap默认容量16，负载因子0.75，基于两数乘积来决定什么时候扩容，第一次扩容按着这个2的幂初始化数组大小，以后每次扩容都是2倍，若没有指定初始值，则为9.96。四、数组与集合数组转集合注意是否使用了视图方式直接返回数组中的数据。我们以Arrays.asList()为例，它把数组转换成集合时，不能使用其修改集合的相关方法，它的add/remove/clear方法会抛出UnsupportedOperationException异常。1234567891011121314151617public static void main(String[] args) &#123; String[] strings = new String[3]; strings[0] = &quot;one&quot;; strings[1] = &quot;two&quot;; strings[2] = &quot;three&quot;; List&lt;String&gt; stringList = Arrays.asList(strings); //修改转换后的集合，成功的把第一个元素&quot;one&quot;变&quot;oneList&quot; stringList.set(0,&quot;oneList&quot;); System.out.println(strings[0]); //以下三个方法编译正确，为什么会抛出运行时异常 stringList.add(&quot;four&quot;); stringList.remove(2); stringList.clear(); //因为asList返回的对象是一个Arrays内部类，并不是真正的ArrayList。&#125;正确使用：12345//正确使用 List&lt;String&gt; stringList1 = new ArrayList&lt;&gt;(Arrays.asList(strings)); stringList1.add(&quot;four&quot;); stringList1.remove(2); stringList1.clear();集合转数组12345678910111213141516171819public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(3); list.add(&quot;one&quot;); list.add(&quot;two&quot;); list.add(&quot;three&quot;); //泛型丢失，无法使用String[]接收返回的结果 Object[] array1 = list.toArray(); //数组长度小于元素长度 String[] array2 = new String[2]; list.toArray(array2); System.out.println(Arrays.asList(array2)); //数组长度等于正确长度 String[] array3 = new String[3]; list.toArray(array3); System.out.println(Arrays.asList(array3));&#125;当数组容量等于集合大小时 运行总是最快的，空间消耗也是最少的。由此证明，如果数组初始大小设 不当，不仅会降低性能，还会浪费空间。使用集合的toArray(T[] array) 方法，转换为数组时 注意需要传入类型完全一样的数组并且它的容量大小为list.size();五、集合与泛型List、List、List&lt;?&gt;的区别？List完全没有类型限制和赋值限定，如果天马行空的乱用，迟早会遭遇类型转换错误；List在接收其他泛型赋值时会编译报错;List&lt;?&gt;是一个泛型，在没有赋值之前可以接收任何类型的集合赋值，赋值之后就不能随便添加了。1234567891011121314151617181920212223242526272829public static void main(String[] args) &#123; //第一段：泛型出现之前的集合定义方式 List a1 = new ArrayList(); a1.add(new Object()); a1.add(new Integer(111)); a1.add(new String(&quot;hello a1a1&quot;)); //第二段：把a1引用赋值给a2，注意a2和a1的区别是增加了泛型限制&lt;Object&gt; List&lt;Object&gt; a2 = a1; a2.add(new Object()); a2.add(new Integer(222)); a2.add(new String(&quot;hello a2a2&quot;)); //第三段：把a1引用赋值给a3，注意a3和a1的区别就是增加了泛型&lt;Integer&gt; List&lt;Integer&gt; a3 = a1; a3.add(new Integer(333)); //下面两行出错，不允许加入非Integer的元素加入集合 a3.add(new Object()); a3.add(new String(&quot;hello a3a3&quot;)); //第四段：把a1引用赋值给a4，a1与a4的区别是增加了通配符 List&lt;?&gt; a4 = a1; //运行删除和清楚元素 a1.remove(0); a4.clear(); //编译出错，不允许增加任何元素 a4.add(new Object()); &#125;&lt;? extends T&gt;与&lt;? super T&gt;的区别？&lt;? extends T&gt;是Get First，适用于消费集合元素的场景，称为上界通配符，可以赋值给任何T或者T的子类的集合，上界为T，取出来的类型带有泛型限制，向上转型为T。&lt;? super T&gt;可以赋值给任何T及T父类的集合，下界为T，称为下界通配符。例如选举代表时，你只能往里投票，取票时，根本不知道谁的票，相当于泛型丢失。extends的场景是put功能受限，而super的场景是get功能受限。六、元素比较归并排序：原理图：代码实现：12345678910111213141516171819202122232425262728293031323334353637//两路归并算法，两个排好序的子序列合并为一个子序列 public void merge(int []a,int left,int mid,int right)&#123; int []tmp=new int[a.length];//辅助数组 int p1=left,p2=mid+1,k=left;//p1、p2是检测指针，k是存放指针 while(p1&lt;=mid &amp;&amp; p2&lt;=right)&#123; if(a[p1]&lt;=a[p2]) tmp[k++]=a[p1++]; else tmp[k++]=a[p2++]; &#125; while(p1&lt;=mid) tmp[k++]=a[p1++];//如果第一个序列未检测完，直接将后面所有元素加到合并的序列中 while(p2&lt;=right) tmp[k++]=a[p2++];//同上 //复制回原素组 for (int i = left; i &lt;=right; i++) a[i]=tmp[i]; &#125; public void mergeSort(int [] a,int start,int end)&#123; if(start&lt;end)&#123;//当子序列中只有一个元素时结束递归 int mid=(start+end)/2;//划分子序列 mergeSort(a, start, mid);//对左侧子序列进行递归排序 mergeSort(a, mid+1, end);//对右侧子序列进行递归排序 merge(a, start, mid, end);//合并 &#125; &#125; @Test public void test()&#123; int[] a = &#123; 49, 38, 65, 97, 76, 13, 27, 50 &#125;; mergeSort(a, 0, a.length-1); System.out.println(&quot;排好序的数组：&quot;); for (int e : a) System.out.print(e+&quot; &quot;); &#125;插入排序：代码实现：1234567891011121314151617181920212223242526272829public static void main(String[] args) &#123; int[] arr = &#123;1,4,6,8,2,5,3,7,9&#125;; System.out.println(&quot;数组排序前顺序&quot;); for (int a : arr) &#123; System.out.print(a + &quot; &quot;); &#125; insertSort(arr); System.out.println(&quot;\n数组排序后的顺序&quot;); for (int a : arr) &#123; System.out.print(a + &quot; &quot;); &#125;&#125;/** * 直接插入排序 * @param arr */private static void insertSort(int[] arr)&#123; for (int i = 1; i &lt; arr.length; i++) &#123; //必须i = 1，因为开始从第二个数与第一个数进行比较 int temp = arr[i]; //待比较值 int j = i - 1; //内存循环为待比较值确定其最终位置 for (;j &gt;= 0 &amp;&amp; arr[j] &gt; temp;j--)&#123;//如果待比较值比前一位值小，应该往前插一位 //将大于temp的值整体后移一位 arr[j+1] = arr[j]; &#125; arr[j+1] = temp;//待比较值值比前一位值大，确定最终位置 &#125;&#125;TimSort：结合归并排序和插入排序的优点，相对于传统归并排序，减少了并归次数，相对于插入排序，引入了二分排序概念，提升排序效率。它主要有两个优化：归并排序的分段不再从单个元素开始，而是每次先查找当前最大的排序好的组数片段run，然后对run进行拓展并进利用二分排序，之后将该run与其他已经排好的run进行归并，产生排好序的大run。引入二分排序，即binarySort。二分排序是对插入排序的优化，在排序中不再是从后往前逐个排序，而是引入了二分查找的思想，将一次查找新元素合适位置的时间复杂度从O(n)降低到O(logn)。hashCode和equals用来标识对象。两个方法协同工作可以用来判断对象是否相等。对象通过Object.hashCode()生成哈希值，由于不可避免地存在哈希值冲突的情况，因此当hashCode相同时，还需要再调用equals进行一次值的比较。如果hashCode不同，直接返回Obejcts不同，跳过equals，加快冲突处理效率，Object定义要求如下：如果两个对象的equals值相等，则两个对象的hashCode的返回结果也一定相等。任何时候重写equals，都必须要重写hashCode。1234567891011121314151617181920212223242526272829303132333435public class Client &#123; private int id; private String name; public Client(int id, String name) &#123; this.id = id; this.name = name; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Client client = (Client) o; return id == client.id &amp;&amp; Objects.equals(name, client.name); &#125; @Override public int hashCode() &#123; return Objects.hash(id, name); &#125; public static void main(String[] args) &#123; Set&lt;Client&gt; hashSet = new HashSet&lt;&gt;(); Client a = new Client(1,&quot;one&quot;); Client b = new Client(1,&quot;one&quot;); Client c = new Client(1,&quot;one&quot;); hashSet.add(a); hashSet.add(b); hashSet.add(c); System.out.println(hashSet.size()); &#125;&#125;七、fail-fast机制集合常见的错误检查机制，通常出现在遍历集合元素的过程中。在遍历途中出现意料之外的修改时，通过unchecked异常暴力反馈出来，这种机制常常出现在多线程环境下，当前线程会维护一个计数比较器，即expectedModCount，记录已经修改的次数。在进入遍历前，会把实时修改次数modCount赋值给expectedModCount，如果两个数据不相等，则抛出异常。java.util下所有的集合包都是fail-fast。123456789101112131415161718192021222324252627282930public static void main(String[] args) &#123; List masterList = new ArrayList(); masterList.add(&quot;one&quot;); masterList.add(&quot;two&quot;); masterList.add(&quot;three&quot;); masterList.add(&quot;four&quot;); masterList.add(&quot;five&quot;); List branchList = masterList.subList(0,3); //下面三行代码如果不注释掉，会导致branchList操作出现异常 masterList.remove(0); masterList.add(&quot;ten&quot;); masterList.clear(); //下面四行全部能执行 branchList.clear(); branchList.add(&quot;sex&quot;); branchList.add(&quot;seven&quot;); branchList.remove(0); //正常遍历，只有一个元素：seven for (Object o: branchList) &#123; System.out.println(o); &#125; //子列表修改导致主列表页被改动，输出：seven，four，five System.out.println(masterList); &#125;concurrent包中所有的集合类都是fail-safe，是在安全的副本上进行遍历，集合修改与副本遍历没有任何关系，但是缺点很明显，就是读取不到最新的数据。Copy-On-Write它是并发的一种新思路，实行读写分离，如果是写操作，则复制一个新集合，在新集合内添加或删除元素。待一切修改完成后，再将原集合的引用指向新的集合，这样做的好处是可以高并发地对COW进行读和遍历操作，而不需要加锁，因为当前集合不会添加任何元素。使用COW时应该注意两点：尽量设置合理的容量初始值，它扩容的代价比较大使用批量添加或者删除方法，如addAll或removeAll操作，在高并发请求下，可以攒一下要添加或者删除的元素，避免增加一个元素复制整个集合。如果集合数据是100MB，再写入50MB，那么某个时间段内占用的内存就达到了（100MB x 2) + 50MB = 250MB,内存大量占用会导致GC的频繁发生，从而降低服务器性能。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4.异常和日志]]></title>
    <url>%2F4.%E5%BC%82%E5%B8%B8%E5%92%8C%E6%97%A5%E5%BF%97.html</url>
    <content type="text"><![CDATA[一、异常分类JDK中定义了一套完整的异常机制，所有异常都是Throwable的子类，主要有分类如下：Error(致命异常)一种非常特殊的异常，它的出现标识着系统发生了不可控制的错误，例如StackOverflowErro、OutOfMemoryErro。针对这种错误，程序无法处理，只能人工介入。Exception(非致命异常)checked异常(受检异常)需要在代码中显示处理的异常，否则会编译出错。如果能自行处理可以在当前方法中捕获异常，如果无法处理，则继续向调用方法抛出异常，常见的异常主要有SQLException、ClassNotFoundException等。unchecked异常(运行时异常)，他们都继承自RuntimeException，不需要程序进行显式的捕捉和处理。进一步可细分3类：可预测异常,常见的包括IndexOutOfBoundsException，NullPointerException等，基于代码性能和稳定性要求，应该做出提前边界检查，空指针判断等处理。需捕获异常,例如在使用Dubbo框架进行RPC调用时产生的远程服务器超时异常DubboTimeoutException，此类异常客户端必须显示处理，可以是重试或者降级处理。可透出异常，主要是框架或系统产生会自行处理的异常，程序无需关系，比如调转404页面。二、异常处理try代码块，例：123456789pub ic static int finallyNotWork () &#123; int temp = 10000 ; try &#123; throw new Excepti on (); &#125; catch (Exception e ) &#123; return ++temp ;&#125; finally&#123; temp = 99999； &#125;如果没有进入进入finally代码块执行，那么有三种可能：没有进入try代码块。进入try代码块，但是出现了死循环或者死锁。进入try代码块，但是执行了System.exit()操作。注意：finally是在return表达式运行后执行的，此时将要return的结果暂时被保存起来，待finally代码块执行结束后再将之前保存的结果返回，切勿在finally中执行赋值操作。自定义异常，继承Exception或者是RuntimeException，在业务中使用throw new XxxException（“xxx错误”）来使用。例如：123456public class XxxException extends RuntimeException&#123;public XxxException()&#123;&#125;public XxxException(String msg) &#123; super(msg);&#125; &#125;三、日志日志级别：DEBUG级别：记录对调试程序有帮助的信息。INFO级别：用来记录程序运行现场，虽然此处并未发生错误，但是对排查其他错误具有指导意义。WARN级别：也可以用来记录程序运行现场，但是更偏向于表明此处有出现潜在错误的可能。ERROR级别：表明当前程序运行发生了错误，需要被关注。但是当前发生的错误，没有影响系统的继续运行。FATAL级别：表明当前程序运行出现了严重的错误事件，并且将会导致应用程序中断。常见用法：~／／使明条件判断形式if (logger.isDebugEnabled()) {logger.debug (“Erocessing trade with id:”+ id + “and symlbol :” + symbol) ;／／使用占位符形式logger.debug (“Processing trade with id: {} and symbol：{}”,id,symbol);]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>exception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5.走进JVM]]></title>
    <url>%2F5%2C%E8%B5%B0%E8%BF%9BJVM.html</url>
    <content type="text"><![CDATA[一、字节码源码转化成字节码过程：Java源文件-&gt;词法解析-&gt;语法解析-&gt;语义分析-&gt;生成字节码二、类加载过程Java的类加载器是一个运行时核心基础设施模块，主要是在启动之初进行类的Load、Link和Init，即加载、链接、初始化。第一步，Load阶段读取类文件产生二进制流，并转化为特定的数据结构，初步校验cafe babe魔法数、常量池、文件长度、是否有父类，然后创建对应类的java.lang.Class实例。第二步，Link阶段包括验证、准备解析三个步骤。验证是更详细的校验，比如final是否合规、类型是否准确静态变量是否合理、准备阶段是为静态变量分配内存，并设定默认值，解析类和方法确保类与类之间的项目引用准确性，完成内存结构布局。Init阶段执行类构造器方法，如果赋值与那算是通过其他类的静态方法来完成的，那么会马上解析另外一个类，在虚拟机栈中执行完毕后通过返回值进行赋值。三、自定义加载器与双亲委派模型什么是双亲委派模型？如果一个类加载器收到类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器完成。每个类加载器都是如此，只有的那个父加载类在自己的搜索范围内找不到指定的类时，自加载器才会尝试自己去加载。自定义类加载器loadClass默认实现如下：123public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException &#123; return loadClass(name, false);&#125;再看看loadClass(String name, boolean resolve)函数：12345678910111213141516171819202122232425262728293031323334353637protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException&#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded Class c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125;&#125;从上面面代码可以明显看出，loadClass(String,boolean)函数即是实现了双亲委派模型，大致过程如下：首先、检查一下指定名称的类是否已经加载过了，如果加载过了，就不需要加载，直接返回。如果此类没有加载过，那么，再判断下是否有父加载器；如果有，则由父加载器加载，或者调用bootstrap类加载器加载。如果父加载器及bootstrap类加载器都没有找到指定的类，则调用当前类findClass方法来完成类加载。换句话说，如果要自定义类加载器，就必要要重写findClas方法。默认实现如下：123protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException&#123; throw new ClassNotFoundException(name);&#125;若果是读取一个指定的名称的类为字节数组，则使用defineClass转换成Class对象，默认实现如下：1234protected final Class&lt;?&gt; defineClass(String name, byte[] b, int off, int len) throws ClassFormatError &#123; return defineClass(name, b, off, len, null);&#125;函数调用过程：开始-&gt;loadClass-&gt;父类加载器是否返回Class对象？是的话直接返回，否-&gt;findClass-&gt;根据名称读取文件存入字节数组-&gt;defineClas-&gt;返回Clas对象。四、内存布局主要有类加载子系统、java栈、方法区、Java堆、直接内存、本地方法栈、垃圾回收机制、PC寄存器、执行引擎。类加载系统和方法区：类加载系统负责从文件系统或者网络中加载class信息，加载的类信息存放在一块称为方法区的内存空间。除了类的信息外，方法区还有可能存放运行时常量池信息，包括字符串常量和数字常量（这部分常量信息是class文件中常量池部分的内存映射）。Java堆：Java堆在虚拟机启动的时候建立，它是Java程序中最重要的内存工作区域。几乎所有的java对象实例都存放在java堆中。堆空间的所有线程是共享的，这是一块与java应用密切相关的内存空间。直接内存：java的NIO库允许java程序使用直接内存。直接内存是java堆外的，直接向系统申请的工作空间。通常访问直接内存的速度会优于java堆。因此出于性能的考虑，读写频繁的场合可能会考虑使用直接内存。由于直接内存在java堆外，因此它的大小不会受限于xmx指定的最大堆大小，但是系统内存是有限的，java堆和直接内存的总和依然受限于操作系统能给出的最大内存。垃圾回收系统：垃圾回收系统是Java虚拟机的重要组成部分，垃圾回收器可以对方法区、java堆和直接内存进行回收。其中，java堆是垃圾收集器的工作重点。和C/C++不同，java中所有的对象空间释放都是隐式的，也就是说，java中没有类似free()或者delete()这样的函数释放指定的内存区域。对于不再使用的垃圾对象，垃圾回收系统会在后台默默工作，默默的查找，标识并释放垃圾对象，完成包括java堆，直接内存和方法区中全自动化管理。Java栈：每一个java虚拟机线程都有一个私有的java栈，一个线程的java栈在线程创建的时候被创建，java栈中保存着帧的信息，java栈中保存着局部变量、方法参数、同时和java方法的调用、返回密切相关。本地方法栈：本地方法栈和java栈非常类似，最大的不同在于java栈用于方法的调用，而本地方法栈则用于本地方法的调用，作为对java虚拟机的重要拓展，java虚拟机允许java直接调用本地方法（通常使用C编写）。PC寄存器：PC寄存器也是每一个线程私有的空间，java虚拟机会为每一个java线程创建PC寄存器。在任意时刻，一个Java线程总是在执行一个方法，这个正在执行的方法称为当前方法。如果当前方法不是本地方法，PC寄存器就会指向正在被执行的指令。如果当前方法是本地方法，那么PC寄存器的值就是undefined。执行引擎：执行引擎是Java虚拟机的最核心组件之一，它负责执行虚拟机的字节码文件，现代虚拟机为了提高执行效率，会使用即时编译技术将方法编译成机器码后执行。五、垃圾回收分代策略JVM内存分代策略Java虚拟机根据对象存活的周期不同，把堆内存划分为几块，一般分为新生代、老年代和永久代（对HotSpot虚拟机而言），这就是JVM内存分代策略。为什么要分代？堆内存是虚拟机管理的内存中最大的一块，也是垃圾回收机制最频繁的一块区域，我们程序所有的对象实例都存放在堆内存中，给堆内存分代是为了提高对象内存分配和垃圾回收的效率。试想一下，如果堆内存没有区域划分，所有的新创建的对象和生命周期很长的对象放在一起，随着程序的执行，堆内存需要频繁进行垃圾回收，而每次回收都要遍历所有对象，遍历这些对象所花费的时间代价是巨大的，会严重我们的GC效率，这简直太可怕了。有了内存分代，情况就不同了，新创建的对象会在新生代中分配内存，经历过多次回收任然存活下来的对象存放在老年代中，静态属性，类信息存放在永久代中，新生代中的对象存活时间短，只需要在新生代区域中频繁进行GC，老年代中对象生命周期长，内存回收的频率相对较低，不需要频繁进行回收，永久代中回收的效果太差，一般不进行垃圾回收，还可以根据不同年代的特点采用合适的垃圾回收算法。分代收集大大提升了收集效率，这些都是内存分代带来的好处。内存分代划分Java虚拟机将堆内存划分为新生代、老年代和永久代，永久代是HotSpot虚拟机特有的概念，它采用永久代的方式来实现方法区，其他的虚拟机实现没有这一个概念，而且HotSpot也有取消永久代的趋势，在JDK1.7中HotSpot已经开始去“永久化”，把原本放在永久代的字符串常量池移出。永久代主要存放常量、类信息、静态变量等数据，与垃圾回收关系不大，新生代和老年代是垃圾回收的主要区域。新生代新生成的对象优先存放在新生代中，新生代对象朝生夕死，存活率很低，在新生代中，常规应用进行一次垃圾收集一般可以回收70%~95%的空间，回收效率很高。HoSpot将新生代划分为三块，一块较大的Eden空间和两块较小的Survivor空间，默认比例是8：1：1。划分的目的是因为HotSpot采用主从复制算法来回收新生代，设计这个比例是为了充分利用内存空间，减少浪费。新生成的对象在Eden区分配（大对象除外，大对象直接进入老年代），当Eden区没有足够的空间进行分配时，虚拟机将发起一次Minor GC。GC开始时，对象只会存在于Eden区和From Survivor区，To Survivor区是空的（作为保留区）。GC进行时，Eden区所有存活的对象都会被复制到To Survivor区，而在From Survivor区中，任然存活的对象会根据它们的年龄值决定去向，年龄值达到年龄阀值（默认是15，新生代中的对象每熬过一次垃圾回收，年龄值就加一，GC分代年龄储存在对象的header中）的对象就会被移到老年区，没有达到阀值的对象都在To Survivor区。接着清空Eden区和From Survivor区，新生代中存活的对象都在To Survivor区，接着From Survivor区和To Survivor区会交换他们的角色，不管怎样都会保证To Survivor区在一轮GC后是空的。GC时当To Survivor区没有足够的空间存放上一次新生代收集下来的存活对象，需要依赖老年代进行分配担，将这些对象放在老年代区。老年代在新生代中经历了多次GC后任然存活下来的对象会进入老年代中。老年代中的对象生命周期较长，在老年代中进行GC的频率相对而言较低，而且回收的速度较慢。永久代永久代存储类信息、常量、静态变量、即时编译器编译之后的代码等数据，对这一区域而言，Java虚拟机规范指出可以不进行垃圾回收收集，一般而言不会进行垃圾回收。回收算法引用计数法比较古老的算法，原理是此对象有一个引用，既增加一个计数，删除一个引用则减少一个计数。垃圾回收时，只用收集计数为0的对象，此算法最致命的是无法处理循环引用的问题。复制*此算法把内存空间分配成两个相等的区域，每次只使用其中的一个区域。垃圾回收时，遍历当前的使用区域，把正在使用的对象复制在另一个区域中，算法每次只处理正在使用中的对象，因此复制都成本较小，同时复制过去还能进行相应的内存整理，不会出现“碎片”问题，当然，此算法的缺点也是很明显的，就是需要两倍的内存空间。标记-清除此算法执行分两阶段，第一阶段从引用根节点开始标记所有被引用的对象，第二阶段遍历整个堆，把未标记的对象清除。此算法需要暂停整个应用，同时，会产生内存碎片。标记-整理此算法整合了“标记-清除”和“复制”两个算法的优点，也是分两个阶段，第一个阶段从根节点开始标记所有被引用的对象，第二个阶段遍历整个堆，清除未标记的对象并且把存活的对象“压缩”到堆的其中一块，按顺序排放。此算法避免了“标记-清除”的碎片问题，同时也避免了”复制“算法的空间问题。垃圾回收器Scavenge GC（次收集）和Full GC的区别（全收集）新生代GC（Scavenge GC）：指发生在新生代的GC，因此新生代的Java对象大多都是朝生夕死，所以Scaveng GC非常频繁，一般回收速度也比较快。当Eden空间不足以为对象分配内存时，会触发Scavenge GC。老年代GC（Full GC/Major GC）：Full GC指发生在老年代的GC，出现了Full GC一般都伴随着至少一次的Minor GC（老年代的对象大部分是Minor GC过程中从新生代进入老年代），比如：分配担保失败，Full GC的速度一般会比Minor GC慢10倍以上。当老年代内存不足或者显式调用System.gc()方法时，会触发Full GC。次收集当年轻的空间紧张时会被触发相对于全收集来说，收集间隔较短全收集当老年代或者持久代堆空间满了，会触发全收集操作可以使用System.gc()方法来显式的启动全收集全收集一般根据堆大小的不同，需要的时间不尽相同，但是一般会比较长，不过，如果全收集时间超过了3到5秒钟，那就太长了。新生代收集器串行收集器（Serial）Serial收集器HotSpot运行在client模式下默认的新生代收集器，它的特点是只有一个cpu/一条收集线程去完成GC工作，且在进行垃圾收集时必须暂停其它所有的工作线程。新生代采用复制算法，老年代采用标记-整理算法。并行收集器（ParNew）ParNew是Serial的多线程版本，除了使用多线程进行GC之外，包括Serial可用的所有控制参数、收集算法、STW、对象分配规则、回收策略等都与Serial完全一样，由于存在线程切换的开销，ParNew在单cpu的环境中比不上Serial，但是随着线程的增加，效率会大大增加。新生代采用复制算法，老年代采用标记-整理算法。Parallel Scavenge收集器与ParNew类似，也是使用复制算法，也是并行多线程收集器，但与其他收集器关注尽可能缩短垃圾收集时间不同，Parallel Scavenge收集器更关注系统吞吐量。系统吞吐量=运行用户代码时间/（运行用户代码时间+垃圾收集时间）。停顿时间越短越适用于用户交互的程序，而高吞吐量则适用于后台运算而不需要太多交互的任务，可以最高效的利用cpu时间，尽快地完成程序的运算任务。老年代收集器erial Old收集器Serial Old是Serial收集器的老年代版本，同样是单线程收集器，使用标记-整理算法。Parallel Old收集器Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和标记-整理算法，吞吐量优先，主要是和Parallel Scavenge在注重吞吐量及cpu资源敏感系统内使用。CMS收集器CMS是一个具有跨时代的收集器，一款真正意义上的并发收集器，虽然已经有了理论意义上表现更好的G1收集器，但现在主流互联网企业线上仍然是CMS。CMS是一种以获取最短回收停顿时间为目标的收集器，又称为多并发低暂停的收集器，基于标记-清除算法实现，整个GC过程分为以下4个步骤：初始标记；并发标记；重新标记；并发清除。由于CMD采用标记-清除算法实现，可能会产生大量的内存碎片，内存碎片过多会导致无法分配大对象而提前触发Full GC，因此CMS提供了-xx：+UseCMSCompactAtFullCollection开关参数。用于在Full GC之后再执行一次碎片整理过程，但是内存整理是无法并发的，内存碎片问题虽然没有了， 但是停顿时间也因此变长了，因此CMS还提供了一个参数-xx：CMSFullGCsBeforeCompaction用于设置在执行N次不进行内存整理的Full GC后，跟着来一次带整理的。分区收集-G1收集器（Garbage-First）G1是一款面向服务端应用的收集器，主要目标用于配备多频cpu的服务器治理大内存–XX:+UseG1GC，启动G1收集与其他基于分代的收集器不同，G1将整个Java堆划分分为多个大小相等的独立区域，虽然还保留着新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[8.Java泛型知识点]]></title>
    <url>%2F8.%E6%B3%9B%E5%9E%8B.html</url>
    <content type="text"><![CDATA[一、什么是泛型？本质就是类型参数化，可以定义在类、接口、方法中，编辑器通过识别尖括号和尖括号内的字母来解析泛型。在定义泛型时，约定俗称的符号包括：E代表Element，用户集合中的元素；T代表the type of object，表示某个类；K代表Key，V代表Value，用户键值对元素。二、使用泛型的好处类型安全。放置的是什么，取回来的自然是什么，不用担心会抛出ClassCastException异常。提升可读性，从编码阶段就显式地知道泛型集合、泛型方法等处理的对象类型是什么。代码重用。泛型合并了同类型的处理代码，使代码重用度变高。三、常见用法泛型类：1234567891011class DateHolder&lt;T&gt;&#123; T item; public void setData(T t)&#123; this.item = t; &#125; public T getDat()&#123; return this.item; &#125;&#125;泛型方法12345678910111213141516171819class DateHolder&lt;T&gt;&#123; T item; public void setData(T t)&#123; this.item = t; &#125; public T getDat()&#123; return this.item; &#125; /** * 泛型方法 * @param e */ public &lt;E&gt; void PrinterInfo(E e)&#123; system.out.println(e); &#125;&#125;泛型接口1234//定义一个泛型接口public interface Generator&lt;T&gt;&#123; public T next();&#125;四、泛型擦除及其相关内容我们下面看一个例子：12345678910Class&lt;?&gt; class1 = new ArrayList&lt;String&gt;().getClass();Class&lt;?&gt; class2 = new ArrayList&lt;Integer&gt;().getClass();System.out.println(class1);System.out.println(class2);System.out.println(class1.equals(class2);结果： java.util.ArrayList java.util.ArrayList true我们看输出发现，class1和class2居然是同一个类型ArrayList，在运行时我们传入的类型变量String和Integer都被丢掉了。Java语言泛型在设计的时候为了兼容原来的旧代码，Java的泛型机制使用了”擦除机制”。注意：编译器虽然会在编译过程中移除参数的类型信息，但是会保证类或方法内部参数类型的一致性。泛型参数将会被擦除到它的第一个边界，如果没有指明边界，那么类型将会被擦除到Object。类型擦除原理：在编译过程中，类型变量的信息是能拿到的。所以，set方法在编辑器可以做类型检查，非法类型不能通过编译。但是对于get方法，由于擦除机制，运行时的实际引用为Object类型，为了还原返回结果的类型，编辑器在get方法后面添加了类型转换。类型擦除的缺陷和补救措施泛型类型不能显示地运用在运行时类型的操作中，例如：转型、instanceof 和 new。因为在运行中，所有参数的类型都丢失了。类似如下代码则无法通过编译：1234567891011121314public class Erased&lt;T&gt;&#123; private final int SIZE = 100; public static void f(Object arg)&#123; //编译不通过 if(arg instanceof T)&#123; &#125; //编译不通过 T var = new T(); //编译不通过 T[] array = new T[SIZE]; //编译不通过 T[] array = (T) new Object[SIZE]; &#125;&#125;解决措施：类型判断问题123456789101112131415/** * 泛型类型判断封装类 * */class GenericType&lt;T&gt;&#123; Class&lt;?&gt; classType; public GenericType(Class&lt;?&gt; type)&#123; this.classType = type; &#125; public boolean isInstance(Object object)&#123; return classType.inInstance(Object); &#125;&#125;创建类型实例123456789101112131415161718192021222324/** *使用工厂方法来创建实例 * * */interface Factory&lt;T&gt;&#123; T create();&#125;class creater&lt;T&gt;&#123; T instance; public &lt;F extends Factory&lt;T&gt;&gt;T newInstance(F f)&#123; instance = f.create(); return instance; &#125;&#125;class IntegerFactory implements Factory&lt;Integer&gt;&#123; @Override public Integer create()&#123; Integer integer = new Integer(9); return integer &#125;&#125;创建泛型数组一般不建议创建泛型数组，尽量使用ArrayList来代替泛型数组。五、Java泛型的通配符上界通配符&lt;? extends T&gt;:只适合频繁读取的场景，例：1234class Food&#123;&#125;class Fruit extends Food&#123;&#125;class Apple extends Fruit&#123;&#125;class Banana extends Fruit&#123;&#125;在上面这个层次，可以用Plate&lt;? extends Fruit&gt;,无法存放，因为编译器只知道容器里存放的是Fruit和它的派生类，不知道具体类，但是可以进行读取操作。下界通配符&lt;? super T&gt;:不影响往里面存储，但是读取出来的数据只能是Object类型，里面存储的都是T及其基类，无法转换成任何一种类型，只能转换成Object基类才能放下。&lt;?&gt;无限通配符无界通配符修饰的容器持有的是某种具体的类型。举个例子，在List&lt;？&gt;类型的引用中，不能向其中添加Object，而在List类型的引用中，可以添加Object对象。PECS原则：上界&lt;? extends T&gt;不能往里存，只能往外取，适合频繁往外面读取内容的场景。下界&lt;? super T&gt;不影响往里存，但往外取只能放在Object对象里，适合经常往里面插入数据的场景。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>泛型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.HTTP/HTTPS]]></title>
    <url>%2F3..http%E5%92%8Chttps.html</url>
    <content type="text"><![CDATA[一、什么是HTTP和HTTPS？HTTP：超文本传输协议，所有的www文件都必须遵守这个标准，互联网上应用最广泛的一种网络协议。HTTPS:是以安全为目标的HTTP通道，简单来说就是HTTP的安全办，即HTTP下加入SLL层，HTTPS的安全基础是SLL，因此加密的详细内容就需要要SSL。主要作用：建立一个信息安全通道，保证数据安全；确定网站真实性。二、两者区别http免费试用，https协议需要ca申请证书，一般免费证书很少，需要交费。http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议。http和https使用的完全不一样的端口，前者是80，后者是443。http的连接很简单，是无状态的。https协议是由ssl+http协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全。三、HTTPS工作流程客户端使用https的URL访问Web服务器，要求与Web服务器建立SSL连接；Web服务器接收客户端请求后，会将网站的证书信息（证书中包含公钥）传送一份给客户端；客户端的浏览器与Web服务器开始协商SSL连接的安全等级，也就是信息加密的等级；客户端浏览器根据双方同意的安全等级，建立会话密钥，然后利用网站的公钥将会话密钥加密，并传送给网站；Web服务器利用自己的私钥解密会话密钥；Web服务器利用会话密钥加密与客户端之间的通信。四、一句话总结HTTPSHTTPS要使客户端与服务器端的通信过程得到安全的保证。必须使用对称加密，但是协商对称密钥的过程，需要使用非对称加密算法来保证安全，然而直接使用非对称加密的过程本身就不安全，会有中间人篡改公钥的可能性，所以客户端与服务器不直接使用公钥，而是使用数字证书签发机构颁发的证书来保证非对称加密过程本身的安全。这样通过这种机制协商出一个对称加密算法，之后双方再使用该算法进行加解密，从而解决了客户端与服务端之间的通信安全问题。五、常见问题TLS和SLL的区别？TLS可以理解成SSL协议3.0版本的升级，所以TLS的1.0版本也被标识为SSL3.1版本。什么是对称加密和非对称加密？对称加密：最快速、最简单的加密方式，加密与解密同样的密钥，这种方法在密码学中叫做对称加密算法。最大缺点是密钥的管理和分配，在发送密钥的过程中，密钥有很大的风险会被黑客拦截。通常做法是将对称加密的密钥进行非对称加密，然后传输给需要他的人。非对称加密：为数据加密与解密提供了一个非常安全的方法，它使用了一对密钥，公钥和私钥。私钥只能由一方保管，不能外泄，而公钥则可以发送给任何请求它的人。非对称加密使用这对密钥中的一个进行加密，而解密则需要另一个密钥。比如，你向银行请求公钥，银行将公钥发送给你，你使用公钥对数据加密，只有私钥的持有人银行才能对你的消息进行解密，安全性大大提高。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.网络安全]]></title>
    <url>%2F2.%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8.html</url>
    <content type="text"><![CDATA[一、黑客与安全黑客：攻击手段分为非破坏性攻击和破坏性攻击。非破坏型攻击：一般是为了扰乱系统的运行，使之暂时失去对外提供服务的能力，比如DDos攻击。破坏性攻击：主要会造成两种后果:系统数据收送或者信息被窃取，比如CSRF攻击。攻击手段有病毒式、洪水式、系统漏洞式。安全：互联网企业都要建立一套完整的信息安全体系，遵循CIA原则，即保密性、完整性、可用性。保密性：对需要保护的数据（例如用户的私人信息等）进行保密操作，无论是存储还是传输，都要保证用户数据及相关资源的安全。完整性：访问的数据需要是完整的，而不是缺失或者篡改的，必然用户访问的数据就是不正确的。可用性：服务必须是可用的。二、SQL注入SQL注入是注入式攻击中常见的类型，是未将代码与数据进行严格的隔离，导致在读取用户数据时候，错误的把数据作为代码的一部分执行，从而导致安全问题。常见案例：123var testCondition;testCondition = Request.from(&quot;testCondition&quot;)var sql = &quot;select * from TableA where id =&apos;&quot;+ testCondition +&quot;&apos;&quot;;上面例子，若果用户只输入ID是一个数字是没有问题的，但是如果用”;”隔开，在testCondition中插入其他SQL，则会带来意想不到的结果。如何预防？过滤用户输入参数中的特殊字符，从而降低被SQL注入的风险；禁止通过字符串凭借的SQL语句，严格使用参数绑定传入的SQL参数；合理使用数据库访问框架提供的防注入机制。例如Mybatis提供的#{} 绑定数据，从而防止SQL注入。同时谨慎使用${},${}相当于使用字符串拼接SQL，拒绝拼接的SQL语句，使用参数化的语句。三、XSS与CSRFXSS:跨站脚本攻击，指黑客通过技术手段，向正常用户请求的HTML页面中插入恶意的脚本，从而可以执行任意脚本,比如如下代码可能造成XSS漏洞12345&lt;div&gt;&lt;h3&gt;反射型xss示例&lt;h3&gt;&lt;br&gt;用户：&lt;%= request.getParameter(&quot;userName&quot;) %&gt;&lt;br&gt;系统错误信息：&lt;%= request.getParameter(&quot;errorMessage&quot;) %&gt;&lt;/div&gt;上面的代码从HTTP请求中获取了userName和errorMessage两个参数，并直接输出到HTML中展示，当黑客构造如下的URL时出现了反射型XSS1http://xss.demo/self-xss.jsp?userName= 张三&lt;script&gt;alert(&quot;张三&quot;)&lt;/script&gt;&amp;errorMessage=XSS示例&lt;script src=http://hacker.demo/xss-script.js /&gt;防范措施：使用Jsonp框架对用户输入字符串做XSS过滤；使用框架的工具类对用户输入的字符串做HTML转义，例如Spring提供的HtmlUtils；前端展示数据时使用innerText而不是innerHTML。CSRF：跨站请求伪造，在用户不知情的情况下，冒用用户发起请求，在当前已经登录的Web应用上执行恶意操作，如恶意发帖，修改密码等。比如某用户A登录了网上银行，这时黑客给他发了一条连接如下：1https//net_bank.demo/transfer.do?targetAccount=12345&amp;amount=100如果用户在打开网银的浏览器中点开了黑客发送的URL,那么就有可能给黑客转账100元。防范措施：CSRF Token验证，利用浏览器的同源限制，在HTTP接口执行前验证页面或者Cookie中设置的Token，只有验证通过才能继续执行请求；人机交互，比如在调用网上银行转账时校验短信验证码。四、两者差别XSS是在正常用户请求的HTML页面中执行了黑客提供的恶意代码；CSRF是黑客直接盗用浏览器中的登录信息，冒充用户去执行黑客指定的操作。XSS问题出在用户数据没有过滤、转义；CSRF问题出现在HTTP接口没有防范不受信任的调用。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>socket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1.什么是TCP/IP]]></title>
    <url>%2F1.TCP-IP.html</url>
    <content type="text"><![CDATA[一、网络协议TCP/IP :中文翻译为传输控制协议，通常有4层协议：应用层、传输层、网络层、链路层。链路层：主要有IEEE 802.x/PPP 等，以字节为单位把0与1进行分组，定义数据帧，写入源和目标机器的物理地址、数据、校验位来传输数据。网络层：主要有IP/APR等，根据IP定义网络地址，区分网段。子网内根据地址解析协议（ARP）进行MAC寻址，子网外进行路由转发数据包，这个数据包即为IP数据包。传输层：主要有TPC/UDP等，数据包通过网络层发送到目标计算机后，应用程序在传输层定义逻辑端口，确认身份，把数据包交给应用程序，实现端口与端口间的通信。应用层：主要有HTTP/FTP/SMTP等，传输层的数据到达应用程序后，以某种统一规定的协议格式解读数据。二、IP协议IP协议是面向无连接、无状态、没有额外的机制保证发送的包是否有序到达。是TCP/IP的基石，几乎所有其他协议都是建立在IP所提供的服务基础上进行传输，其中包括在实际应用中用于传输稳定有序数据的TCP。三、TPC建立链接传输控制协议。是一种面向连接、确保数据在端与端之间可靠传输的协议。三次握手:A机器发出一个数据包并将SYN置1，表示希望建立连接；B机器收到A机器发过来的数据包，通过SYN得知这是一个建立连接的请求，于是发送一个响应包并将SYN和ACK标记都置为1。假设这个包中的序列号是y，而确认序列号必须是x+1，表示收到了A发过来的SYN，在TCP中，SYN被当做数据部分的一个字节；A收到B的响应包后进行确认，确认包中将ACK置为1，并将确认序列号置为y+1，表示收到来自B的SYN。目的： 信息对等，防止超时。四、TCP断开连接四次挥手：A机器想要关闭连接，则待本方数据发送完毕之后，传递FIN信号给B机器；B机器应答ACK，告诉A机器可以断开，但是要等B机器处理完数据，再主动给A机器发送FIN信号，此时A机器处于半关闭状态，无法发送新的数据；B机器做好链接关闭的准备后，发送FIN信号给A机器，此时B机器也进入半关闭状态；A机器发送针对B机器FIN的ACK后，进入TIME-WAIT状态，经过2MSL后，没有收到B发送的报文，则确定B机器已经收到A机器最后发送的ACK命令，此时TCP正式释放。五、常见问题1. TCP和UDP区别？TCP:面向连接点到点通信高可靠性占用系统资源多、效率低利用IO流实现数据的传输响应式请求UDP:非面向连接，传输不可靠，可能丢失发送不管对方是否准备好，接受到也不确认可以广播发送非常简单的协议，开销小效率高，不用IO流实现数据的传输2. 为什么连接的时候是三次握手，断开的时候是四次挥手？当服务端收到客户端的连接请求报文时，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的，但是关闭连接时候，当服务端收到FIN报文时，不会立刻关闭SOCKET，只能先回复一个ACK报文，告诉客户端你的FIN报文收到了，只有等我服务端所有的报文发送完，我才能发送FIN报文，因此不能一次发送，需要四步。3.为什么不用两次握手链接？容易发生死锁，客户端在服务端的应答分组在传输中被丢失的情况下，将不知道服务端是否准备好，不知道服务端建立什么样的序列号，在这种情况下，客户端认为链接还未建立，将忽略服务端发来的任何数据分组，只能等待链接确认才应答f分组，而服务端发出的分组超时后，重复发送同样的分组，这样就行成了死锁。4.为什么TIME-WAIT状态需要经过2MSL才能返回到Close状态？按照道理来说，四个报文发送完毕，我们可以直接进入CLOSE状态，但是我们假象网络是不可靠的，有可能最后一个ACK丢失，所以TIME-WAIT状态是用来重发可能丢失的ACK报文。在客户端发送出最后的ACK回复，但该ACK可能丢失，服务端如果没有收到ACK，则不断的发送FIN片段，所以客户端不能立即关闭，它必须确认客户端收到了该ACK，此时设置了一个定时器，如果直到2MSL，客户端都没有收到FIN，则推断ACK已经成功被接收，关闭连接。5.如果已经建立了连接，但是客户端突然出现故障怎么办？TPC设有一个保活计时器，服务端每次接收到请求都会重新复位这个计时器，时间通常是2小时，若2小时没有收到客户端的任何数据，则服务端会发送一个探测报文段，以后每隔75秒发送一次，若一连发送10个探测报文都没有反应，则认为客户端发生故障，关闭连接。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>socket</tag>
      </tags>
  </entry>
</search>
